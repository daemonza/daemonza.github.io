<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>https://daemonza.github.io/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <managingEditor>werner.gillmer@gmail.com (Werner Gillmer)</managingEditor>
    <webMaster>werner.gillmer@gmail.com (Werner Gillmer)</webMaster>
    <copyright>(c) 2017 Werner Gillmer</copyright>
    <lastBuildDate>Mon, 13 Feb 2017 09:42:32 +0100</lastBuildDate>
    <atom:link href="https://daemonza.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Kubernetes nginx-ingress-controller</title>
      <link>https://daemonza.github.io/2017/02/13/kubernetes-nginx-ingress-controller/</link>
      <pubDate>Mon, 13 Feb 2017 09:42:32 +0100</pubDate>
      <author>werner.gillmer@gmail.com (Werner Gillmer)</author>
      <guid>https://daemonza.github.io/2017/02/13/kubernetes-nginx-ingress-controller/</guid>
      <description>

&lt;h4 id=&#34;introduction:fddc9cbd6401bed2470db90b3d9bacf5&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;In this post I will explain, how I expose applications running on Kubernetes clusters to the internet with the
help of Ingress controllers.&lt;/p&gt;

&lt;p&gt;But first a little bit about Kubernetes Ingresses and Services. On a very simplistic level a &lt;code&gt;Service&lt;/code&gt; is a
logical abstraction communication layer to pods. During normal operations pods get&amp;rsquo;s created, destroyed, scaled out, etc.&lt;/p&gt;

&lt;p&gt;A &lt;code&gt;Service&lt;/code&gt; make&amp;rsquo;s it easy to always connect to the pods by connecting to their &lt;code&gt;service&lt;/code&gt; which stays stable during the
pod life cycle. A important thing about &lt;code&gt;services&lt;/code&gt; are what their type is, it determines how the &lt;code&gt;service&lt;/code&gt; expose itself
to the cluster or the internet. Some of the &lt;code&gt;service&lt;/code&gt; types are :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ClusterIP
Your service is only expose internally to the cluster on the internal cluster IP. A example would be to
deploy Hasicorp&amp;rsquo;s vault and expose it only internally.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;NodePort
Expose the service on the EC2 Instance on the specified port. This will be exposed to the internet. Off course it
this all depends on your AWS Security group / VPN rules.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;LoadBalancer
Supported on Amazon and Google cloud, this creates the cloud providers your using load balancer. So on Amazon it creates
a ELB that points to your &lt;code&gt;service&lt;/code&gt; on your cluster.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ExternalName
Create a CNAME dns record to a external domain.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information about &lt;code&gt;Services&lt;/code&gt; look at &lt;a href=&#34;https://kubernetes.io/docs/user-guide/services/&#34;&gt;https://kubernetes.io/docs/user-guide/services/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A &lt;code&gt;Ingress&lt;/code&gt; is rules on how to access a &lt;code&gt;Service&lt;/code&gt; from the internet.&lt;/p&gt;

&lt;p&gt;For more information about &lt;code&gt;Ingresses&lt;/code&gt; look at
&lt;a href=&#34;https://kubernetes.io/docs/user-guide/ingress/&#34;&gt;https://kubernetes.io/docs/user-guide/ingress/&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;scenario:fddc9cbd6401bed2470db90b3d9bacf5&#34;&gt;Scenario&lt;/h4&gt;

&lt;p&gt;Imagine this scenario, you have a cluster running, on Amazon, you have multiple applications deployed to it, some are API&amp;rsquo;s some
are Java applications running inside something like Tomcat, and to add to the mix, you have a couple of static html pages sitting in a
Apache web server that serves as documentation for your api&amp;rsquo;s. All applications needs to have SSL, some of the api&amp;rsquo;s endpoints have changed,
but you still have to serve the old endpoint path, so you need to do some sort of path rewrite. How do you expose everything to the
internet? The obvious answer is create a  &lt;code&gt;type LoadBalancer&lt;/code&gt; service for each, but, then multiple ELB&amp;rsquo;s will be created, you have to
deal with SSL termination at each ELB, you have to CNAME your applications/api&amp;rsquo;s domain names to the right ELB&amp;rsquo;s, and in general just have
very little control over the ELB.&lt;/p&gt;

&lt;p&gt;Enter &lt;code&gt;Ingress Controllers&lt;/code&gt;. You deploy a &lt;code&gt;ingress controller&lt;/code&gt;, create a &lt;code&gt;type LoadBalancer&lt;/code&gt; service for it, and it sits and monitors
Kubernetes api server&amp;rsquo;s /ingresses endpoint and acts as a reverse proxy for the ingress rules it found there. You then deploy
your application and expose it&amp;rsquo;s service as a &lt;code&gt;type NodePort&lt;/code&gt;, and create ingress rules for it. The &lt;code&gt;ingress controller&lt;/code&gt; then
picks up the new deployed service and proxy traffic to it from outside.&lt;/p&gt;

&lt;p&gt;Following this setup, you only have one ELB then on Amazon, and a central place at the &lt;code&gt;ingress controller&lt;/code&gt; to manage the traffic coming
into your cluster to your applications.&lt;/p&gt;

&lt;h4 id=&#34;ingress-controllers:fddc9cbd6401bed2470db90b3d9bacf5&#34;&gt;Ingress controllers&lt;/h4&gt;

&lt;p&gt;Two of the more popular ones that I normally use, are Traefik and Nginx-ingress-controller. I like both, and for simple projects I prefer using Traefik
as it&amp;rsquo; easy to setup and use. However as the projects needs grow or for more complex projects, I reach for the nginx-ingress-controller.&lt;/p&gt;

&lt;p&gt;Couple of differences between the two, that normally helps me decide which one to pick for a project.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Traefik routes to the Kubernetes service, Nginx, by pass the service and routes to the pods. This allows nginx
to do sticky sessions if your application needs it.&lt;/li&gt;
&lt;li&gt;Traefik works with multiple backends (docker, swarm, marathon, etcd, consul, kubernetes, etc. etc.) See &lt;a href=&#34;https://docs.traefik.io/&#34;&gt;https://docs.traefik.io/&lt;/a&gt;
The nginx-ingress-controller is specfic for Kubernetes, you can off course build your own Nginx reverse proxy, perhaps with OpenResty
and get it to work with almost any backend, but that requires a significant time investment.&lt;/li&gt;
&lt;li&gt;The nginx-ingress-controller can handle websockets, Traefik does not.&lt;/li&gt;
&lt;li&gt;Traefik can do basic path manipulation by using &lt;code&gt;PathPrefixStrip&lt;/code&gt; but cannot do more complex path rewrites like Nginx.&lt;/li&gt;
&lt;li&gt;The nginx-ingress-controller is build on top of OpenResty (&lt;a href=&#34;https://openresty.org&#34;&gt;https://openresty.org&lt;/a&gt;). Using Lua you can easily extend Nginx capabilities
and mold it to do whatever you need it to do.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information look at :&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://traefik.io/&#34;&gt;https://traefik.io/&lt;/a&gt;
&lt;a href=&#34;https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx&#34;&gt;https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In general I prefer the nginx-ingress-controller, so for the rest of the article I will focus on it.&lt;/p&gt;

&lt;h4 id=&#34;setup:fddc9cbd6401bed2470db90b3d9bacf5&#34;&gt;Setup&lt;/h4&gt;

&lt;p&gt;Let&amp;rsquo;s setup a little demo &amp;lsquo;hello world&amp;rsquo; api on our  cluster and expose the api on the internet using
nginx-ingress-controller. Let&amp;rsquo;s deploy the &lt;code&gt;nginx-ingress-controller&lt;/code&gt; first.&lt;/p&gt;

&lt;p&gt;For the controller, the first thing we need to do is setup a default backend service for
nginx.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;default backend&lt;/code&gt; is the default service that nginx falls backs to if if cannot route a request
successfully. The &lt;code&gt;default backend&lt;/code&gt; needs to satisfy the following two requirements :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;serves a 404 page at /&lt;/li&gt;
&lt;li&gt;serves 200 on a /healthz&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See more here &lt;a href=&#34;https://github.com/kubernetes/contrib/tree/master/404-server&#34;&gt;https://github.com/kubernetes/contrib/tree/master/404-server&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s use the example &lt;code&gt;default backend&lt;/code&gt; from the nginx-ingress-controller github project&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f https://raw.githubusercontent.com/kubernetes/contrib/master/ingress/controllers/nginx/examples/default-backend.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we want to handle SSL requests (which we should always do) Nginx needs to have a default SSL certificate. This
get&amp;rsquo;s used for requests for there is not specified SSL certificate.&lt;/p&gt;

&lt;p&gt;Assuming you have SSL cert and key, create &lt;code&gt;secrets&lt;/code&gt; as follow, where tsl.key is the key name
and tsl.crt is your certificate and dhparam.pem is the pem file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create secret tls tls-certificate --key tls.key --cert tls.crt 
kubectl create secret generic tls-dhparam --from-file=dhparam.pem 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save the following to a file (e.g. nginx-ingress-controller.yml) :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
spec:
  type: LoadBalancer
  ports:
    - port: 80
      name: http
    - port: 443
      name: https
  selector:
    k8s-app: nginx-ingress-lb
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 2
  revisionHistoryLimit: 3
  template:
    metadata:
      labels:
        k8s-app: nginx-ingress-lb
    spec:
      terminationGracePeriodSeconds: 60
      containers:
        - name: nginx-ingress-controller
          image: gcr.io/google_containers/nginx-ingress-controller:0.8.3
          imagePullPolicy: Always
          readinessProbe:
            httpGet:
              path: /healthz
              port: 18080
              scheme: HTTP
          livenessProbe:
            httpGet:
              path: /healthz
              port: 18080
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 5
          args:
            - /nginx-ingress-controller
            - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
            - --default-ssl-certificate=$(POD_NAMESPACE)/tls-certificate
          # Use downward API
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - containerPort: 80
            - containerPort: 443
          volumeMounts:
            - name: tls-dhparam-vol
              mountPath: /etc/nginx-ssl/dhparam
            - name: nginx-template-volume
              mountPath: /etc/nginx/template
              readOnly: true
      volumes:
        - name: tls-dhparam-vol
          secret:
            secretName: tls-dhparam
        - name: nginx-template-volume
          configMap:
            name: nginx-template
            items:
            - key: nginx.tmpl
              path: nginx.tmpl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You might have noticed as well, we specify a &lt;code&gt;default-backend-service&lt;/code&gt;, this is used by nginx to route requests on
which it found not ingress rule match. We need to deploy this &lt;code&gt;default-backend-service&lt;/code&gt; before we deploy
nginx. We can use the example default backend yaml file from the Kubernetes ingress github repo. After the deployment
expose the &lt;code&gt;default-http-backend&lt;/code&gt; so that the nginx-ingress-controller can communicate with it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f https://raw.githubusercontent.com/kubernetes/contrib/master/ingress/controllers/nginx/examples/default-backend.yaml
kubectl expose rc default-http-backend --port=80 --target-port=8080 --name=default-http-backend
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now deploy &lt;code&gt;nginx-ingress-controller&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./nginx-ingress-controller.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What this command did was, it created a deployment with two replicas of the &lt;code&gt;nginx-ingress-controller&lt;/code&gt;
and a service for it of &lt;code&gt;type LoadBalancer&lt;/code&gt; which created a ELB for us on AWS.
Let&amp;rsquo;s confirm that. Get the service :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get services -o wide | grep nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should get something similar to the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nginx-ingress  100.62.254.11  aaba00ec3d35211r68caa0a32e7f202e-566d19265.eu-west-1.elb.amazonaws.com  80:32199/TCP,443:31772/TCP 25d  k8s-app=nginx-ingress-lb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This means the ELB on Amazon got created. Any domain&amp;rsquo;s who&amp;rsquo;s traffic we want to go to this
ingress controller should be CNAMED to this ELB hostname (&lt;code&gt;aaba00ec3d35211r68caa0a32e7f202e-566d19265.eu-west-1.elb.amazonaws.com&lt;/code&gt;)
For the rest of this post, I am going to assume api.daemonza.io is CNAMED to this ELB&lt;/p&gt;

&lt;p&gt;Make sure the deployment pods are up and running :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get pods | grep nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should get something similar to the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nginx-ingress-controller-1461575992-62j00   1/1       Running   0          30sec
nginx-ingress-controller-1461575992-eoqqb   1/1       Running   0          30sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using the ELB hostname that we got from the querying for the &lt;code&gt;nginx-ingress-controller&lt;/code&gt; service, make
sure that traffic is being routed.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v aaba00ec3d35211r68caa0a32e7f202e-566d19265.eu-west-1.elb.amazonaws.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HTTP/1.1 404 Not Found
Server: nginx/1.11.3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the response should be&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;default backend - 404
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This means everything is working correctly and the ELB forwarded traffic to our
&lt;code&gt;nginx-ingress-controller&lt;/code&gt; and the &lt;code&gt;nginx-ingress-controller&lt;/code&gt; passed it along to the
&lt;code&gt;default-backend-service&lt;/code&gt; that we deployed.&lt;/p&gt;

&lt;h4 id=&#34;deploying-our-api-application:fddc9cbd6401bed2470db90b3d9bacf5&#34;&gt;Deploying our API application&lt;/h4&gt;

&lt;p&gt;I like Go, so I created a little API that we can deploy and test the ingress controller with.
This is the api we are going to deploy :
&lt;a href=&#34;https://hub.docker.com/r/daemonza/testapi/&#34;&gt;https://hub.docker.com/r/daemonza/testapi/&lt;/a&gt;
and the super simplistic source code is at &lt;a href=&#34;https://github.com/daemonza/testapi&#34;&gt;https://github.com/daemonza/testapi&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Create a file, let&amp;rsquo;s call it testapi.md with the following contents&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1 
kind: Deployment 
metadata:
  name: testapi 
  namespace: default 
  labels:
    application: testapi
spec:
  replicas: 3
  selector:
    matchLabels:
      application: testapi
  template:
    metadata:
      labels:
        application: testapi 
    spec:
      containers:
        - name: testapi
          image: daemonza/testapi:latest
          imagePullPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: testapi 
  namespace: default 
  labels:
    application: testapi 
spec:
  type: NodePort
  selector:
    application: testapi 
  ports:
  - port: 8080
    targetPort: 8080 
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: testapi
  namespace: default
  annotations:
    kubernetes.io/ingress.class: &amp;quot;nginx&amp;quot;
    ingress.kubernetes.io/ssl-redirect: &amp;quot;true&amp;quot;
spec:
  rules:
  - host: &amp;quot;api.daemonza.io&amp;quot;
    http:
      paths:
      - path: /testapi
        backend:
          serviceName: testapi
          servicePort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Quick run through of the above file. It consists of three parts, deployment, service, ingress.&lt;/p&gt;

&lt;p&gt;I normally split these into threes separate files, but for the sake of this blog post, let&amp;rsquo;s
keep it all in one file. For the nginx-ingress-controller we are mostly interested in
the &lt;code&gt;ingress&lt;/code&gt; part.&lt;/p&gt;

&lt;p&gt;In the form of &lt;code&gt;annotations&lt;/code&gt; it&amp;rsquo;s possible for us to pass some configuration through to the nginx-ingress-controller.
In the above example we say we want this to always be used with nginx-ingress-controller, in case we have multiple
controllers, and we always want to redirect HTTP to HTTPS.
Then we want to route all traffic where the host (virtual host) is api.daemonza.io but only on the &lt;code&gt;/testapi&lt;/code&gt; path, and
we want to route the traffic to the &lt;code&gt;testapi service&lt;/code&gt;, which in turn expose the &lt;code&gt;testapi deployment&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If we look at the equivalent simplified nginx configuration it would look similar to the following, where the &lt;code&gt;upstream&lt;/code&gt; is
&lt;code&gt;deployment&lt;/code&gt; with three replica&amp;rsquo;s.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    upstream default-testapi-8080 {
        server 100.96.3.148:8080 max_fails=0 fail_timeout=0;
        server 100.96.4.222:8080 max_fails=0 fail_timeout=0;
        server 100.96.4.223:8080 max_fails=0 fail_timeout=0;   
    }
    
    server {
            server_name api.daemonza.io;
            listen 80;

            location /testapi {
                   
                proxy_pass http://default-testapi-8080;
            }
            
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s test it.
Deploy with the following command&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./testapi.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see output similar to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;deployment &amp;quot;testapi&amp;quot; created
service &amp;quot;testapi&amp;quot; created
ingress &amp;quot;testapi&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s check on the ingress rule&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get ingress testapi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see something similar to :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME      HOSTS             ADDRESS          PORTS     AGE
testapi   api.daemonza.io   51.232.218.155   80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s ask for a little more information with &lt;code&gt;describe&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl describe ingress testapi


Name:			testapi
Namespace:		default
Address:		52.214.207.195
Default backend:	default-http-backend:80 (&amp;lt;none&amp;gt;)
Rules:
  Host			Path	Backends
  ----			----	--------
  api.daemonza.io
    			/testapi 	testapi:8080 (&amp;lt;none&amp;gt;)
Annotations:
  ssl-redirect:	true
Events:
  FirstSeen	LastSeen	Count	From				SubObjectPath	Type		Reason	Message
  ---------	--------	-----	----				-------------	--------	------	-------
  2m		2m		1	{nginx-ingress-controller }			Normal		CREATE	default/testapi
  2m		2m		1	{nginx-ingress-controller }			Normal		CREATE	ip: 51.232.218.155
  2m		2m		1	{nginx-ingress-controller }			Normal		UPDATE	default/testapi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From this we can see that we asked the nginx-ingress-controller to route requests
for &lt;code&gt;api.daemonza.io&lt;/code&gt; on path &lt;code&gt;/testapi&lt;/code&gt; to our backend &lt;code&gt;testapi&lt;/code&gt;. Plain HTTP requests
will be redirected over to HTTPS and the testapi is scaled out to three replicas.&lt;/p&gt;

&lt;p&gt;Ok so with the testapi deployed, let&amp;rsquo;s see if everything works.
The test api expose the following endpoints :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PUT /put/:something
GET /get/:something
POST /post/:something
DELETE /get/:something
PATCH /patch/:something
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where :something is anything you want to put there.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s try a POST request&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -X POST http://api.daemonza.io/post/hello
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We should get http status code 301(moved permanently) back, this is because in our
testapi ingress specification, we added this annotation :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ingress.kubernetes.io/ssl-redirect: &amp;quot;true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which tells nginx to redirect http to https.
If we do the request again on https we should get status 200 back. Assuming that worked, let&amp;rsquo;s move on and add
some virtual hosts for our testapi.&lt;/p&gt;

&lt;h4 id=&#34;virtual-hosts:fddc9cbd6401bed2470db90b3d9bacf5&#34;&gt;Virtual hosts&lt;/h4&gt;

&lt;p&gt;Change the testapi.md ingress specification to look as follow, change the host domain name to a domain that you own
or setup them up in your local /etc/hosts file for testing.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: testapi
  namespace: default
  annotations:
    kubernetes.io/ingress.class: &amp;quot;nginx&amp;quot;
    ingress.kubernetes.io/ssl-redirect: &amp;quot;true&amp;quot;
spec:
  rules:
  - host: &amp;quot;blue.daemonza.io&amp;quot;
    http:
      paths:
      - path: /testapi
        backend:
          serviceName: testapi
          servicePort: 8080  
  - host: &amp;quot;green.daemonza.io&amp;quot;
    http:
      paths:
      - path: /testapi
        backend:
          serviceName: testapi
          servicePort: 8080  
  - host: &amp;quot;api.myfakedomain.io&amp;quot;
    http:
      paths:
      - path: /testapi
        backend:
          serviceName: testapi
          servicePort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also route multiple virtual hosts on different paths to the same backend service.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: testapi
  namespace: default
  annotations:
    kubernetes.io/ingress.class: &amp;quot;nginx&amp;quot;
    ingress.kubernetes.io/ssl-redirect: &amp;quot;true&amp;quot;
spec:
  rules:
  - host: &amp;quot;blue.daemonza.io&amp;quot;
    http:
      paths:
      - path: /testapi/get/blue
        backend:
          serviceName: testapi
          servicePort: 8080
  - host: &amp;quot;green.daemonza.io&amp;quot;
    http:
      paths:
      - path: /testapi/get/green
        backend:
          serviceName: testapi
          servicePort: 8080                 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the above the example we are routing two different virtual hosts, to the same API, but on different
api endpoints. We can then off course also do the opposite by having multiple little api &amp;ldquo;micro services&amp;rdquo;
and route traffic to the correct one by matching up the backend serviceName with the Path.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s test requests to our api on the api.myfakedomain.io virtual host we added.
Add api.myfakedomain.io to your /etc/hosts file in order to test with that domain.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl https://api.myfakedomain.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running that curl command, curls tells us &lt;code&gt;curl: (60) SSL certificate problem: Invalid certificate chain&lt;/code&gt;
which is correct as we don&amp;rsquo;t have the SSL certificate for that domain configured in nginx. Luckily through
ingress rules, we can specify a ssl certificate per virtual host as follow :&lt;/p&gt;

&lt;p&gt;First thing we need to do is create a Kubernetes secret containing our SSL certificate and key&lt;/p&gt;

&lt;p&gt;Add the following to a file, let&amp;rsquo;s call is apitest_ssl.md&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
kind: Secret
metadata:
  name: apitestsecret
  namespace: default
type: Opaque
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then deploy it to Kubernetes with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./apitest_ssl.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modify the apitest.md file ingress specification for api.myfakedomain.io to reference this secret as follow :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spec:
  tls:
  - hosts:
    - &amp;quot;api.myfakedomain.io&amp;quot;
    secretName: apitestsecret
  rules:
  - host: &amp;quot;api.myfakedomain.io&amp;quot;
    http:
      paths:
      - path: /testapi
        backend:
          serviceName: testapi
          servicePort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now if you ask for the api.myfakedomain.io ingress rules&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl describe ingress api.myfakedomain.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;you should get a similar reply to the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Name:			api.myfakedomain.io
Namespace:		default
Address:		52.223.113.173,52.224.52.153
Default backend:	default-http-backend:80 (&amp;lt;none&amp;gt;)
TLS:
  apitestsecret terminates api.myfakedomain.io
Rules:
  Host			Path	Backends
  ----			----	--------
  api.myfakedomain.io
    			/testapi 	apitest:8080 (&amp;lt;none&amp;gt;)
Annotations:
  ssl-redirect:	true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running the curl command now against &lt;a href=&#34;https://api.fakedomain.io&#34;&gt;https://api.fakedomain.io&lt;/a&gt; should work without any warnings about
insecure certificates.&lt;/p&gt;

&lt;h4 id=&#34;path-rewrites:fddc9cbd6401bed2470db90b3d9bacf5&#34;&gt;Path rewrites&lt;/h4&gt;

&lt;p&gt;Sometimes, there is a need to rewrite the path of a request to match up with the backend service. One such scenario might be,
a API got developed and deployed, got changed over time, but there is still a need to be backwards
compatibility on the API endpoints.&lt;/p&gt;

&lt;p&gt;So using out deployed apitest api, lets setup a path rewrite. We have a &lt;code&gt;/new/get&lt;/code&gt; api path, but the api does
not support it, it supports, &lt;code&gt;/get/new&lt;/code&gt; we now need to rewrite the path to that. Using a annotation with the ingress rules
we can rewrite the path as follow :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: apitest
  namespace: applications
  annotations:
    ingress.kubernetes.io/rewrite-target: /new/get
    kubernetes.io/ingress.class: &amp;quot;nginx&amp;quot;
spec:
  rules:
  - host: &amp;quot;api.daemonza.io&amp;quot;
    http:
      paths:
      - path: /get/new
        backend:
          serviceName: apitest
          servicePort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;sticky-sessions:fddc9cbd6401bed2470db90b3d9bacf5&#34;&gt;Sticky sessions&lt;/h4&gt;

&lt;p&gt;Won&amp;rsquo;t it be nice if all applications got developed from the start with the 12-factor (&lt;a href=&#34;https://12factor.net/&#34;&gt;https://12factor.net/&lt;/a&gt;) methodology
in mind? Unfortunately that does not always happen, and there is times when you have to be able
to handle stateful application on your Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;Luckily for us the nginx-ingress-controller can handle sticky sessions as it bypass the service level and
route directly the pods.&lt;/p&gt;

&lt;p&gt;In order to get sticky sessions to work we need to create a configMap and enable it.&lt;/p&gt;

&lt;p&gt;Save the following in a file, for now call it nginx.conf.md&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
data:
  enable-sticky-sessions: &amp;quot;true&amp;quot;
kind: ConfigMap
metadata:
  name: nginx-ingress-controller-conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and load it with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./nginx.conf.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The controller will reload on configuration change.&lt;/p&gt;

&lt;p&gt;What this setting does it, instruct nginx to use the nginx-sticky-module-ng module
(&lt;a href=&#34;https://bitbucket.org/nginx-goodies/nginx-sticky-module-ng&#34;&gt;https://bitbucket.org/nginx-goodies/nginx-sticky-module-ng&lt;/a&gt;) that&amp;rsquo;s bundled with the controller
to handle all sticky sessions for us.&lt;/p&gt;

&lt;p&gt;This is how the generated nginx configuration looks after enabling sticky sessions.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;upstream default-testapi-8080 {
    sticky name=SERVERID httponly path=/;
    server 100.96.3.148:8080 max_fails=0 fail_timeout=0;
    server 100.96.4.222:8080 max_fails=0 fail_timeout=0;
    server 100.96.4.223:8080 max_fails=0 fail_timeout=0;   
}
    
server {
        server_name api.daemonza.io;
        listen 80;

        location /testapi {           
            proxy_pass http://default-testapi-8080;
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;proxy-protocol:fddc9cbd6401bed2470db90b3d9bacf5&#34;&gt;Proxy protocol&lt;/h4&gt;

&lt;p&gt;Lots of times you need to pass a user&amp;rsquo;s IP address / hostname through to your application. A example would
be, to have the hostname of the user in your application logs.&lt;/p&gt;

&lt;p&gt;By default if you deploy the nginx-ingress-controller on AWS behind a ELB, the ELB will not pass along the hostname
information, to solve this we need to enable &lt;code&gt;proxy protocol&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Add the following to your Nginx configMap.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;use-proxy-protocol: &amp;quot;true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And in your nginx-ingress-controler service specification add the following annotation&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: &#39;*&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will make sure that the ELB that get&amp;rsquo;s created will have &lt;code&gt;proxy protocol&lt;/code&gt; enabled.
If you prefer not to change the ELB from a Kubernetes Service, you can configure it manually on the ELB
by following the documentation here : &lt;a href=&#34;http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html&#34;&gt;http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And excellent explanation of what proxy protocol is and how it works can be found at
&lt;a href=&#34;http://www.haproxy.org/download/1.8/doc/proxy-protocol.txt&#34;&gt;http://www.haproxy.org/download/1.8/doc/proxy-protocol.txt&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;custom-nginx-configuration:fddc9cbd6401bed2470db90b3d9bacf5&#34;&gt;Custom Nginx configuration&lt;/h4&gt;

&lt;p&gt;Sometimes there is a need to configure something in Nginx, that&amp;rsquo;s not possible through the nginx-ingress-controller
configMap, annotations or ingress rules. In such cases, it&amp;rsquo;s possible to edit &lt;code&gt;nginx.tmpl&lt;/code&gt; Go template.&lt;/p&gt;

&lt;p&gt;You can get the nginx.tmpl file from the nginx-ingress-controller Github repository or, copy it over from your
nginx-ingress-controller pod. It&amp;rsquo;s located at &lt;code&gt;/etc/nginx/template/nginx.tmpl&lt;/code&gt;. Make your changes to it
and then save it as a configMap as follow&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create configmap nginx-template --from-file=nginx.tmpl=./nginx.tmpl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then mount the nginx-template configMap as a volume in the your nginx-ingress-controller
specification.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;volumeMounts:
   - name: nginx-template-volume
     mountPath: /etc/nginx/template
      readOnly: true
volumes:
   - name: nginx-template-volume
     configMap:
       name: nginx-template
       items:
       - key: nginx.tmpl
         path: nginx.tmpl
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;conclusion:fddc9cbd6401bed2470db90b3d9bacf5&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;The Kubernetes ingress specifications combined with the nginx-ingress-controller gives a incredible flexible and
powerful routing platform for your Kubernetes clusters. For more information about Kubernetes Ingress and the
Nginx-ingress-controller visit :&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/user-guide/ingress/&#34;&gt;https://kubernetes.io/docs/user-guide/ingress/&lt;/a&gt;
&lt;a href=&#34;https://github.com/nginxinc/kubernetes-ingress/tree/master/nginx-controller&#34;&gt;https://github.com/nginxinc/kubernetes-ingress/tree/master/nginx-controller&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes on AWS</title>
      <link>https://daemonza.github.io/2017/01/15/kubernetes-on-aws/</link>
      <pubDate>Sun, 15 Jan 2017 10:09:57 +0100</pubDate>
      <author>werner.gillmer@gmail.com (Werner Gillmer)</author>
      <guid>https://daemonza.github.io/2017/01/15/kubernetes-on-aws/</guid>
      <description>

&lt;p&gt;In this post I will be explaining how I setup Kubernetes clusters on Amazon.&lt;/p&gt;

&lt;p&gt;I use kops (&lt;a href=&#34;https://github.com/kubernetes/kops&#34;&gt;https://github.com/kubernetes/kops&lt;/a&gt;) to setup and manage my Kubernetes clusters. For my day job, I create
multiple Kubernetes stacks often on Amazon and nothing, so far, comes close to how well kops works for me. I briefly
flirted with kube-aws  (&lt;a href=&#34;https://github.com/coreos/kube-aws&#34;&gt;https://github.com/coreos/kube-aws&lt;/a&gt;) from CoreOS, and while it&amp;rsquo;s a good tool, Kops just works
better for me. Also there is something to be said for using a tool developed and used by the same guys that develop Kubernetes.&lt;/p&gt;

&lt;p&gt;There is also kubeadm (&lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/kubeadm/&#34;&gt;https://kubernetes.io/docs/getting-started-guides/kubeadm/&lt;/a&gt;), however it&amp;rsquo;s currently in alpha. Have not
used it myself before, but it&amp;rsquo;s definitely something I am keeping a eye on.&lt;/p&gt;

&lt;h4 id=&#34;installing-kops:3ac6298d606a83e1b55f117f57e6507d&#34;&gt;Installing kops&lt;/h4&gt;

&lt;p&gt;I use macOS so from here all instructions will be based on the assumption it&amp;rsquo;s being executed on macOS. It should
however work on Linux or at least be easily adapted to work on Linux.&lt;/p&gt;

&lt;p&gt;Install Go 1.7 and install kops from source. The pre-build releases of kops does not include a fix in the Go net lib
which seems to break things in kops every now and again, so we need to build it from source to get the fix.&lt;/p&gt;

&lt;p&gt;Install the following :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Go 1.7 or better from &lt;a href=&#34;https://golang.org/dl/&#34;&gt;https://golang.org/dl/&lt;/a&gt; or use brew&lt;/li&gt;
&lt;li&gt;kops - v1.4.4&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;go get -d k8s.io/kops
cd ${GOPATH}/src/k8s.io/kops/
git checkout release
make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Test that kops works.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops version
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;setup:3ac6298d606a83e1b55f117f57e6507d&#34;&gt;Setup&lt;/h4&gt;

&lt;p&gt;It&amp;rsquo;s strictly not needed to install awscli, as you can do everything needed for setting up a kops created aws kubernetes
cluster through the Amazon web console. However, I find it useful and prefer it to the console.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;aws-cli/1.10.22 (&lt;a href=&#34;http://docs.aws.amazon.com/cli/latest/userguide/installing.html&#34;&gt;http://docs.aws.amazon.com/cli/latest/userguide/installing.html&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Configure your amazon credentials by running :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aws configure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure you don&amp;rsquo;t have &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt; and &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt; exported as environment variables in your
shell as it will take preference over the &lt;code&gt;aws configure&lt;/code&gt; credentials. You can off course skip the aws &lt;code&gt;aws configure&lt;/code&gt;
and just export your AWS credentials with the above mentioned environment variables.&lt;/p&gt;

&lt;p&gt;Create a S3 state store bucket for kops. This bucket get&amp;rsquo;s used by kops to store
information and configuration of all your Kubernetes clusters you create. Anyone with
Amazon credentials to the S3 bucket can create/modify and delete the your Kubernetes clusters, which
makes it very convenient to share this bucket with team members or if you move between computers, and not having to
move configuration, etc. with you.&lt;/p&gt;

&lt;p&gt;Creating a state store bucket in the eu-west-1 region of amazon.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aws s3api create-bucket --bucket my-state-store --region eu-west-1
export KOPS_STATE_STORE=s3://my-state-store
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Decide on a place to save your cluster kubeconfig and ssh keys locally. I like creating a directory structure
that mimics my my clusters. So for example if I am setting a development stack up for myself I would :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p ~/stacks/dev.daemonza.io
touch ~/stacks/dev.daemonza.io/kubeconfig 
EXPORT KUBECONFIG=&amp;quot;~/stacks/dev.daemonza.io/kubeconfig&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Change to this directory and create ssh keys. I normally put the keys in a separate &lt;code&gt;credentials&lt;/code&gt; directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir credentials
ssh-keygen -t rsa # save the keys in the credentials directory
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;create-cluster:3ac6298d606a83e1b55f117f57e6507d&#34;&gt;Create cluster&lt;/h4&gt;

&lt;p&gt;First thing is to decided on is the DNS setup for your cluster. It&amp;rsquo;s standard practice to use sub domains
to identify a cluster. E.g. dev.daemonza., qa.daemonza.io, prod.daemonza.io for the develop, QA and
production stacks. Following that convention let&amp;rsquo;s create our cluster under dev.daemonza.io.
Seeing as we are deploying to AWS, let&amp;rsquo;s use Amazon&amp;rsquo;s Route53 DNS service to handle the DNS side for us.&lt;/p&gt;

&lt;p&gt;If you have not already, point your root domain name, in this case daemonza.io to use Route53. Then
add a NS record to the daemonza.io for the sub domain where this cluster will be(dev.daemonza.io), with the
sub domains NS servers as values.&lt;/p&gt;

&lt;p&gt;First create the sub domain on route 53&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ID=$(uuidgen) &amp;amp;&amp;amp; aws route53 create-hosted-zone --name dev.daemonza.io --caller-reference $ID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Take note of the &amp;ldquo;NameServers&amp;rdquo; returned. It looks like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;NameServers&amp;quot;: [
    &amp;quot;ns-1225.awsdns-00.org&amp;quot;,
    &amp;quot;ns-1821.awsdns-44.co.uk&amp;quot;,
    &amp;quot;ns-820.awsdns-32.net&amp;quot;,
    &amp;quot;ns-143.awsdns-29.com&amp;quot;
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now using the above name servers, create a r53-ns-batch.json file. I save this file
in the locally created stack dir mentioned above.&lt;/p&gt;

&lt;p&gt;Example r53-ns-batch.json file using the above name servers&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    {
      &amp;quot;Comment&amp;quot;: &amp;quot;reference subdomain of dev.daemonza.io cluster&amp;quot;,
      &amp;quot;Changes&amp;quot;: [
        {
          &amp;quot;Action&amp;quot;: &amp;quot;CREATE&amp;quot;,
          &amp;quot;ResourceRecordSet&amp;quot;: {
            &amp;quot;Name&amp;quot;: &amp;quot;dev.daemonza.io&amp;quot;,
            &amp;quot;Type&amp;quot;: &amp;quot;NS&amp;quot;,
            &amp;quot;TTL&amp;quot;: 300,
            &amp;quot;ResourceRecords&amp;quot;: [
              {
                &amp;quot;Value&amp;quot;: &amp;quot;ns-1225.awsdns-00.org&amp;quot;
              },
              {
                &amp;quot;Value&amp;quot;: &amp;quot;ns-1821.awsdns-44.co.uk&amp;quot;
              },
              {
                &amp;quot;Value&amp;quot;: &amp;quot;ns-820.awsdns-32.net&amp;quot;
              },
              {
                &amp;quot;Value&amp;quot;: &amp;quot;ns-143.awsdns-29.com&amp;quot;
              }
            ]
          }
        }
      ]
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now using this r53-ns-batch.json file create a record in your parent domain (daemonza.io).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aws route53 change-resource-record-sets \
    --hosted-zone-id XXXXXXXXXXXX \
    --change-batch file://r53-ns-batch.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;XXXXXXXXXXXX being your your parent domain(daemonza.io) zone id. You can find your domain zone ID using&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aws route53 list-hosted-zones
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Use the following command to get the state of the DNS change&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aws route53 get-change --id CHANGE_ID_FROM_PREVIOUS_COMMAND
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait here until status is INSYNC. It normally takes a couple of moments for me to get
the INSYNC.&lt;/p&gt;

&lt;p&gt;Now, we are ready to create the cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops create cluster --v=0 \
  --cloud=aws \
  --node-count 2 \
  --master-size=t2.medium \
  --master-zones=eu-west-1a \
  --zones eu-west-1a,eu-west-1b \
  --name=dev.daemonza.io \
  --node-size=m3.xlarge \
  --ssh-public-key=~/stacks/dev.daemonza.io/id_rsa.pub \
  --dns-zone dev.daemonza.io \
  2&amp;gt;&amp;amp;1 | tee /stacks/dev.daemonza.io/create_cluster.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A little explanation of the parameters passed to kops :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;--cloud&lt;/code&gt;
We want to install the cluster on Amazon&amp;rsquo;s AWS. At the moment only aws and google cloud is
supported.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--node-count&lt;/code&gt;
The number of worker nodes. This is the nodes running our applications.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--master-size&lt;/code&gt;
The EC2 instance type of the master node (manage Kubernetes)
Kubernetes recommends the following on master node sizing&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;For the master, for clusters of less than 5 nodes it will use an m3.medium, for 6-10 nodes it will use an m3.large;
for 11-100 nodes it will use an m3.xlarge.
For worker nodes, for clusters less than 50 nodes it will use a t2.micro, for clusters between 50 and 150 nodes it
will use a t2.small and for clusters with greater than 150 nodes it will use a t2.medium.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--master-zones&lt;/code&gt;
The AWS availability zone(&lt;a href=&#34;http://docs.aws.amazon.com/general/latest/gr/rande.html&#34;&gt;http://docs.aws.amazon.com/general/latest/gr/rande.html&lt;/a&gt;) where the master EC2
instance will be placed.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--zones&lt;/code&gt;
The AWS availability zones where the worker nodes EC2 instances will be places.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--name&lt;/code&gt;
The name you are giving to your stack. This is dev.daemonza.io for this example.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--node-size&lt;/code&gt;
The worker node EC2 instance type (&lt;a href=&#34;https://aws.amazon.com/ec2/instance-types/&#34;&gt;https://aws.amazon.com/ec2/instance-types/&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--ssh-public-key&lt;/code&gt;
This is the ssh key you generated in the previous command.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--dns-zone&lt;/code&gt;
The fqdn of where your stack will be, I normally make this the same as the &lt;code&gt;name&lt;/code&gt; parameter, so
dev.daemonza.io in this case.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The output of the commmand I pipe to stdout, to monitor progress and activity while creating the stack and to a file
in case I missed something on stdout or want to reference something of the stack create in the future.&lt;/p&gt;

&lt;p&gt;This command generates your Kubernetes cluster configuration. Now run update (see below) to
create your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops update cluster dev.daemonza.io --yes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait a while for your cluster to come up. You can look in the AWS console at the EC2 instances
being created. It takes roughly around 10 mins or so for your cluster to become available.&lt;/p&gt;

&lt;p&gt;You can confirm the cluster is there by running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will take a while to work, and it will first start working on and off, as in you would get the nodes, or
some of the nodes back, or a error. Wait here until it returns all the nodes (in this case, two workers, and one master)
consistently. This step normally takes me around 15 to 20mins.&lt;/p&gt;

&lt;p&gt;When the cluster is up, I usually deploy the Kubernetes Dashboard (&lt;a href=&#34;https://github.com/kubernetes/dashboard&#34;&gt;https://github.com/kubernetes/dashboard&lt;/a&gt;). While I use
the kubectl command line tool probably 99% of the time, it&amp;rsquo;s nice to every now and again look at your cluster using the UI.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can now run kubectl proxy and access your cluster at
&lt;a href=&#34;http://127.0.0.1:8001/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard/#/deployment?namespace=_all&#34;&gt;http://127.0.0.1:8001/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard/#/deployment?namespace=_all&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;list-clusters:3ac6298d606a83e1b55f117f57e6507d&#34;&gt;List clusters&lt;/h4&gt;

&lt;p&gt;Kops makes it easy to manage multiple stacks. To get a list of all your current stacks
run the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops get clusters
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Example output :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME			CLOUD	ZONES
dev.daemonza.io	aws	eu-west-1a,eu-west-1b
testing.daemonza.io	aws	eu-west-1a,eu-west-1b
production.daemonza.io	aws	eu-west-1a,eu-west-1b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see I have three stacks running. One being the stack (dev.daemonza.io) that we just created, and a testing
and production cluster.
From here we can get various information about a cluster. For example to get a description of the
EC2 instances that make up a cluster run :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops get instancegroups
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command will list the instances of the cluster to which your KUBECONFIG environment variable is
pointing. Earlier in this tutorial we created a kubeconfig file and exported KUECONFIG to that. The kops
create / update command filled out the kubeconfig file for us, which we are using now.&lt;/p&gt;

&lt;p&gt;Example output :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Using cluster from kubectl context: dev.daemonza.io

NAME			ROLE	MACHINETYPE	MIN	MAX	ZONES
master-eu-west-1a	Master	t2.medium	1	1	eu-west-1a
nodes			Node	m3.xlarge	2	2	eu-west-1a,eu-west-1b
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;modify-cluster:3ac6298d606a83e1b55f117f57e6507d&#34;&gt;Modify cluster&lt;/h4&gt;

&lt;p&gt;Kops makes it easy, and relative safe to modify a Kubernetes cluster with it&amp;rsquo;s rolling updates. Continuing from
our example above in the  cluster listing. Let&amp;rsquo;s modify our cluster to have three m3.xlarge worker nodes instead
of just two, and change the root volume size.&lt;/p&gt;

&lt;p&gt;Run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops edit instancegroups nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will open a configuration file in your default editor (set it with &lt;code&gt;export EDITOR=vim&lt;/code&gt; if you want vim as default)&lt;/p&gt;

&lt;p&gt;Example of how the configuration looks :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metadata:
  creationTimestamp: &amp;quot;2017-01-04T13:59:07Z&amp;quot;
  name: nodes
spec:
  associatePublicIp: true
  image: kope.io/k8s-1.4-debian-jessie-amd64-hvm-ebs-2016-10-21
  machineType: m3.xlarge
  maxSize: 2
  minSize: 2
  role: Node
  zones:
  - eu-west-1a
  - eu-west-1b                                                                                                                                                       13
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Change the maxSize value to 3 and keep minSize on 2, this will set the autoscale group on amazon
to scale to three worker nodes.
And for the root volume add the following for a 100gig volume of type gp2 (for more on EC2 volume types
see &lt;a href=&#34;http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html)&#34;&gt;http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html)&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rootVolumeSize: 100
rootVolumeType: gp2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With your end result looking like the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metadata:
  creationTimestamp: &amp;quot;2017-01-04T13:59:07Z&amp;quot;
  name: nodes
spec:
  associatePublicIp: true
  image: kope.io/k8s-1.4-debian-jessie-amd64-hvm-ebs-2016-10-21
  machineType: m3.xlarge
  maxSize: 2
  minSize: 2
  role: Node
  rootVolumeSize: 100
  rootVolumeType: gp2
  zones:
  - eu-west-1a
  - eu-west-1b 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Confirm the changes is correct by running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops update cluster dev.daemonza.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If your happy with the changes run the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops update cluster dev.daemonza.io --yes
kops rolling-update cluster --yes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sit back and wait for your cluster changes to take affect.&lt;/p&gt;

&lt;h4 id=&#34;remove-cluster:3ac6298d606a83e1b55f117f57e6507d&#34;&gt;Remove cluster&lt;/h4&gt;

&lt;p&gt;Now to remove the cluster that you just created run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops delete cluster dev.daemonza.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will print a list of amazon resources that will be removed.
Example :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TYPE		NAME				ID
dhcp-options	dev.daemonza.io	        	dopt-6627fc02
route-table 	dev.daemonza.io	    	    rtb-36fc1351
security-group	masters.dev.daemonza.io	    sg-e6e99a80
security-group	nodes.dev.daemonza.io	    sg-19e99a7f
subnet		    eu-west-1a.dev.daemonza.io	subnet-1cdbb344
subnet		    eu-west-1b.dev.daemonza.io	subnet-b17f51d5
vpc	        	dev.daemonza.io		        vpc-bbf7a5df

Must specify --yes to delete
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then if you are a 100% sure you want to remove the cluster, NOTE, there is no coming back
from this, add &amp;ndash;yes to the the command.&lt;/p&gt;

&lt;h4 id=&#34;conclusion:3ac6298d606a83e1b55f117f57e6507d&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;That&amp;rsquo;s it folks! Kops is a powerful tool to work with Kubernetes and we have barely scratch the
surface in this article. For more information, of what else you can do with Kops head over to
&lt;a href=&#34;https://github.com/kubernetes/kops&#34;&gt;https://github.com/kubernetes/kops&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>