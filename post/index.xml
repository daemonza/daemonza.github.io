<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on </title>
    <link>https://daemonza.github.io/post/index.xml</link>
    <description>Recent content in Posts on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <managingEditor>werner.gillmer@gmail.com (Werner Gillmer)</managingEditor>
    <webMaster>werner.gillmer@gmail.com (Werner Gillmer)</webMaster>
    <copyright>(c) 2017 Werner Gillmer</copyright>
    <lastBuildDate>Mon, 06 Mar 2017 13:06:32 +0100</lastBuildDate>
    <atom:link href="https://daemonza.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Using Ansible to automate my Macbook setup</title>
      <link>https://daemonza.github.io/2017/03/06/using-ansible-to-automate-my-macbook-setup/</link>
      <pubDate>Mon, 06 Mar 2017 13:06:32 +0100</pubDate>
      <author>werner.gillmer@gmail.com (Werner Gillmer)</author>
      <guid>https://daemonza.github.io/2017/03/06/using-ansible-to-automate-my-macbook-setup/</guid>
      <description>

&lt;h4 id=&#34;introduction&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;I am soon going to get a new Macbook, and have been thinking about how to setup it quickly and easily.
There is &lt;a href=&#34;https://github.com/boxen/boxen&#34;&gt;Boxen&lt;/a&gt;, and while it is awesome at what is does, it does
too much for my needs. A quick Google shows there is plenty of other tools and solutions out there
like &lt;code&gt;Boxen&lt;/code&gt; that automate a Mac setup. While they all looked mostly good, I want something a little more
personal, so, let&amp;rsquo;s reinvent the wheel :D&lt;/p&gt;

&lt;h4 id=&#34;configuration-management&#34;&gt;Configuration management&lt;/h4&gt;

&lt;p&gt;I noticed that &lt;code&gt;Boxen&lt;/code&gt; is build on top of &lt;a href=&#34;https://puppet.com/&#34;&gt;Puppet&lt;/a&gt;, nice, however I prefer
&lt;a href=&#34;https://www.ansible.com/&#34;&gt;Ansible&lt;/a&gt;, which led me the idea of using Ansible to automate my Macbook setup.&lt;/p&gt;

&lt;p&gt;First step, how to install applications. I normally use &lt;a href=&#34;https://brew.sh/&#34;&gt;Homebrew&lt;/a&gt; as much as possible to
install command line utilities. But normally install GUI apps either through the Mac store or from the
developer&amp;rsquo;s website. Which will not be so nice to try and maintain through &lt;code&gt;Ansible&lt;/code&gt;. Looking through brew
and &lt;a href=&#34;https://caskroom.github.io/&#34;&gt;Cask&lt;/a&gt; however I realized that I will be able to install almost everything
I want using just brew and git. awesome!&lt;/p&gt;

&lt;p&gt;Browsing through the Ansible module&amp;rsquo;s, I saw that Homebrew is &lt;a href=&#34;http://docs.ansible.com/ansible/homebrew_module.html&#34;&gt;supported&lt;/a&gt; :D
So no need to use &lt;code&gt;Ansible&lt;/code&gt; command module to call brew! And in normal &lt;code&gt;Ansible&lt;/code&gt; fashion, it&amp;rsquo;s incredibly easy
to use.&lt;/p&gt;

&lt;p&gt;Example snippet of how to install vim using the homebrew module :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- name: install vim 
  homebrew:
    name: vim 
    state: latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and using cask&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- name: install chrome 
  homebrew_cask:
    name: google-chrome 
    state: installed 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How simple is that! So now we have all the building blocks to automate the setup of my macbook.&lt;/p&gt;

&lt;h4 id=&#34;setupmac&#34;&gt;Setupmac&lt;/h4&gt;

&lt;p&gt;So this is what I build :
&lt;a href=&#34;https://github.com/daemonza/setupmac&#34;&gt;https://github.com/daemonza/setupmac&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Yes, I know, super original name :D&lt;/p&gt;

&lt;p&gt;What is does is, it bootstraps getting &lt;code&gt;Ansible&lt;/code&gt; by installing &lt;a href=&#34;https://pypi.python.org/pypi/pip&#34;&gt;pip&lt;/a&gt; with &lt;code&gt;easy_install&lt;/code&gt; which in
turns then install &lt;code&gt;Ansible&lt;/code&gt;. From there, the &lt;code&gt;Ansible&lt;/code&gt; playbook runs a &lt;code&gt;setup&lt;/code&gt; role against localhost that installs &lt;code&gt;homebrew&lt;/code&gt; and update it.
It then installs the applications listed &lt;a href=&#34;https://raw.githubusercontent.com/daemonza/setupmac/master/roles/setup/vars/main.yml&#34;&gt;here&lt;/a&gt;.
Unfortunately two of the applications is not available through Homebrew, so I download those to my &lt;code&gt;$HOME/Downloads&lt;/code&gt; directory. Luckily
this is simple to do with &lt;code&gt;Ansible&lt;/code&gt; :&lt;/p&gt;

&lt;p&gt;Example snippet of how to download a app(&lt;a href=&#34;http://zwift.com/&#34;&gt;Zwift&lt;/a&gt;) to $HOME/Downloads :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   general:
     local_home: &amp;quot;{{ lookup(&#39;env&#39;,&#39;HOME&#39;) }}&amp;quot;

   zwift:
     name: Zwift
     url: http://cdn.zwift.com/app/ZwiftOSX.dmg
     dest: &amp;quot;{{general.local_home}}/Downloads/ZwiftOSX.dmg&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;Ansible&lt;/code&gt; role then starts the configuration, which involves getting &lt;a href=&#34;http://ohmyz.sh/&#34;&gt;oh-my-zsh&lt;/a&gt; with the &lt;a href=&#34;https://draculatheme.com/&#34;&gt;theme&lt;/a&gt;
that I use, and &lt;a href=&#34;http://spacemacs.org/&#34;&gt;spacemacs&lt;/a&gt; from git, clones my &lt;a href=&#34;https://github.com/daemonza/dotfiles&#34;&gt;dotfiles&lt;/a&gt; from my github repository,
and copies the respective files to the correct locations.&lt;/p&gt;

&lt;p&gt;To make it as easy as possible to install, you can execute the start script locally on your mac with curl as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -s https://raw.githubusercontent.com/daemonza/setupmac/master/start.sh | /bin/bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/daemonza/setupmac/master/start.sh&#34;&gt;script&lt;/a&gt; that bootstraps &lt;code&gt;ansible&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;Ansible is awesome! It provides just the right level of power vs complexity to quickly enable you
to build useful automation. While &lt;code&gt;setupmac&lt;/code&gt; is really setup for how I like my Macbook, I hope it will be useful
to someone who wants to do something similar.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Helm to deploy to Kubernetes</title>
      <link>https://daemonza.github.io/2017/02/20/using-helm-to-deploy-to-kubernetes/</link>
      <pubDate>Mon, 20 Feb 2017 15:21:26 +0100</pubDate>
      <author>werner.gillmer@gmail.com (Werner Gillmer)</author>
      <guid>https://daemonza.github.io/2017/02/20/using-helm-to-deploy-to-kubernetes/</guid>
      <description>

&lt;h4 id=&#34;introduction&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;In this post I will be showing how to use Helm (&lt;a href=&#34;https://helm.sh&#34;&gt;https://helm.sh&lt;/a&gt;) to build and deploy your own charts to a Kubernetes
cluster.&lt;/p&gt;

&lt;h4 id=&#34;preface&#34;&gt;Preface&lt;/h4&gt;

&lt;p&gt;What is Helm? Well, think of it as the apt-get / yum of Kubernetes, it&amp;rsquo;s a package manager for Kubernetes
developed by the guys from Deis (&lt;a href=&#34;https://deis.com/&#34;&gt;https://deis.com/&lt;/a&gt;). If you deploy applications to Kubernetes, Helm makes
it incredibly easy to version those deployments, package it, make a release of it, and deploy, delete, upgrade
and even rollback those deployments as charts. Charts being the terminology that &lt;code&gt;helm&lt;/code&gt; use for package of
configured Kubernetes resources.&lt;/p&gt;

&lt;p&gt;There is a second part to Helm and that is &lt;code&gt;Tiller&lt;/code&gt;. Tiller is the Helm server side that runs in Kubernetes
and handles the Helm packages.&lt;/p&gt;

&lt;p&gt;So before we can use &lt;code&gt;helm&lt;/code&gt; with a kubernetes cluster, you need to install &lt;code&gt;tiller&lt;/code&gt; on it. It&amp;rsquo;s as easy
as running :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm init
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;building-a-helm-chart&#34;&gt;Building a Helm chart&lt;/h4&gt;

&lt;p&gt;Let&amp;rsquo;s see Helm in action, using a small little Go test api I created specifically for testing use cases like this, let&amp;rsquo;s
build a helm chart of it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/daemonza/testapi.git; cd testapi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First create a skeleton structure chart&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm create testapi-chart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will create a &lt;code&gt;testapi-chart&lt;/code&gt; directory. Inside this directory the three files we are the most interested in for is
Chart.yaml, values.yaml and NOTES.txt.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Chart.yaml describes the chart, as in it&amp;rsquo;s name, description and version.&lt;/li&gt;
&lt;li&gt;values.yaml is stores variables for the template files templates directory. If you have more complex deployment needs, that
falls outside the default templates capability, edit the files in this directory. They are normal Go templates, Hugo (&lt;a href=&#34;https://gohugo.io&#34;&gt;https://gohugo.io&lt;/a&gt;)
which btw powers this blog, have a nice Go template primer (&lt;a href=&#34;https://gohugo.io/templates/go-templates/&#34;&gt;https://gohugo.io/templates/go-templates/&lt;/a&gt;), if you need more information
on how to work with Go templates.&lt;/li&gt;
&lt;li&gt;NOTES.txt is used to give information after deployment to the user that deployed the chart. For example it might explain how
to use the chart, or list default settings, etc. For this post I will keep the default message in it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Open Chart.yaml and fill out the details of the application your deploying. Using the &lt;code&gt;testapi&lt;/code&gt; as a example, this is
how my Chart.yaml looks like :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
description: A simple api for testing and debugging
name: testapi-chart
version: 0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now open &lt;code&gt;values.yaml&lt;/code&gt; and edit it as needed. Again, using the &lt;code&gt;testapi&lt;/code&gt; as a example this is how my &lt;code&gt;values.yaml&lt;/code&gt; file
looks like.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;replicaCount: 2
image:
  repository: daemonza/testapi
  tag: latest
  pullPolicy: IfNotPresent
service:
  name: testapi
  type: ClusterIP
  externalPort: 80
  internalPort: 80
resources:
  limits:
    cpu: 100m
    memory: 128Mi
  requests:
    cpu: 100m
    memory: 128Mi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; helm lint
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;in your testapi_chart directory to make sure everything is ok.
If everything is good, you can package the chart as a release by running :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm package testapi-chart --debug
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I like to add the &lt;code&gt;--debug&lt;/code&gt; flag to see the output of the packaged chart.
Output should look similar to the following&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Saved /Users/daemonza/testapi/testapi-chart/testapi-chart-0.0.1.tgz to current directory
Saved /Users/daemonza/testapi/testapi-chart/testapi-chart-0.0.1.tgz to /Users/daemonza/.helm/repository/local
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From that we can see that the chart is placed in our current directory as well as in our local
helm repository.
To deploy this release, we can point helm directly to the chart file as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm install testapi-chart-0.1.0.tgz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And your output should look similar to the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME:   ordered-quoll
LAST DEPLOYED: Wed Mar  1 09:39:48 2017
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&amp;gt; v1/Service
NAME                       CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
ordered-quoll-testapi-ch   10.0.0.133   &amp;lt;none&amp;gt;        80/TCP    0s

==&amp;gt; extensions/Deployment
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
ordered-quoll-testapi-ch   2         2         2            0           0s


NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l &amp;quot;app=ordered-quoll-testapi-ch&amp;quot; -o jsonpath=&amp;quot;{.items[0].metadata.name}&amp;quot;)
  echo &amp;quot;Visit http://127.0.0.1:8080 to use your application&amp;quot;
  kubectl port-forward $POD_NAME 8080:80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From the above we can see that a &lt;code&gt;deployment&lt;/code&gt; was created in kubernetes, the testapi got scaled to two pods and a &lt;code&gt;service&lt;/code&gt;
got created to expose the &lt;code&gt;deployment&lt;/code&gt; on the cluster IP on port 80. And the NOTES.txt file tells us how to access the
pod.&lt;/p&gt;

&lt;p&gt;List the deployed packages with their release versions by running :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which should return output similar to the following&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME        	REVISION	UPDATED                 	STATUS  	CHART
ordered-quoll	1       	Wed Mar  1 11:48:52 2017	DEPLOYED	testapi-chart-0.1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modify the &lt;code&gt;Chart.yaml&lt;/code&gt; file and change the version from 0.1.0 to 0.1.1 package and deploy the 0.1.1 chart.
Running &lt;code&gt;helm ls&lt;/code&gt; again now shows us that we have two packages of the testapi deployed&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ordered-quoll	1       	Wed Mar  1 11:48:52 2017	DEPLOYED	testapi-chart-0.1.0
wishful-ibis	1       	Wed Mar  1 12:03:31 2017	DEPLOYED	testapi-chart-0.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s confirm that the testapi indeed deployed, and that there is two versions of it running :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get deployments
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should have output similar to :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
ordered-quoll-testapi-cha   1         1         1            1          11m
wishful-ibis-testapi-cha    1         1         1            1           3m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Time to get rid of the older 0.1.0 deployment of the testapi chart.
Using it&amp;rsquo;s package name from &lt;code&gt;helm ls&lt;/code&gt; run :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm delete ordered-quoll
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Confirm again with &lt;code&gt;kubectl get deployments&lt;/code&gt; that it really is removed.
Expected output similar to :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
wishful-ibis-testapi-cha   1         1         1            1           7m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As we can see the &lt;code&gt;ordered-quoll&lt;/code&gt; 0.1.0 version is removed. But what happens if we want to
go back. Imagine this scenario, we deployed a new version of a application, and for some reason its
got a problem and we need to rollback to the previous version. No worries, &lt;code&gt;helm&lt;/code&gt; got our back.
Simply run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm rollback ordered-quoll 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which means we want to rollback the testapi package ordered-quoll one revision back.&lt;/p&gt;

&lt;p&gt;Expected output&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Rollback was a success! Happy Helming!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But what, if you cannot remember what the name was of a deleted package? Or just want to see all the packages that&amp;rsquo;s been
deleted? No problem, run :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm ls --deleted
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And if you want to see it ordered by date just add a &lt;code&gt;-d&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We have really gone a round about way of deploying packages so far with helm, normally you would only want to
upgrade a package, instead of deploying a new version alongside it, unless off course your following a blue / green
style deployment process.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s test upgrading a release. Open the testapi project, edit the Chart.yaml as a example and change the description,
and version number, package the release with &lt;code&gt;helm package .&lt;/code&gt;, but now instead of using &lt;code&gt;helm install&lt;/code&gt; run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm upgrade ordered-quoll .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will upgrade our &lt;code&gt;ordered-quoll&lt;/code&gt; release to the changes we just made. Running &lt;code&gt;helm ls&lt;/code&gt; now, you
should see the new version next to &lt;code&gt;ordered-quoll&lt;/code&gt;. And off course we can rollback this release as before.&lt;/p&gt;

&lt;p&gt;For more information on using  &lt;code&gt;Helm&lt;/code&gt; look at
&lt;a href=&#34;https://github.com/kubernetes/helm/blob/master/docs/using_helm.md&#34;&gt;https://github.com/kubernetes/helm/blob/master/docs/using_helm.md&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;While this works well, it would be nicer to put our &lt;code&gt;chart&lt;/code&gt; in a Helm repository, as it&amp;rsquo;s then easy to share
the chart or access it from other clusters, etc.&lt;/p&gt;

&lt;h4 id=&#34;setting-up-helm-repository&#34;&gt;Setting up Helm repository&lt;/h4&gt;

&lt;p&gt;A Helm repository is nothing more than just a web server that&amp;rsquo;s able to serve a index.yaml file
and chart files, which is really just tar.gz file containing the generated kubernetes resource manifest files from our helm chart
templates. So almost any web server will do. For local testing
you can also use the helm command itself. Here is a example of helm serving the charts from a &lt;code&gt;charts&lt;/code&gt;
directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm serve --repo-path ./charts
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Problem is you need to get your charts onto the web server somehow, and there is a myriad amount of solutions
on how to do it, it can as a example be as simple as just using &lt;code&gt;scp&lt;/code&gt; to get your chart to the web server, or setting up
Caddy(&lt;a href=&#34;https://caddyserver.com/&#34;&gt;https://caddyserver.com/&lt;/a&gt;) with the Upload plugin(&lt;a href=&#34;https://caddyserver.com/docs/upload&#34;&gt;https://caddyserver.com/docs/upload&lt;/a&gt;). You can also use AWS S3 or
a Google GCS bucket as well to host a chart repository, which does make uploading easier, by using the google cloud command
line utility or the UI or for S3 use one of the many S3 tools out there. I personally prefer s3cmd (&lt;a href=&#34;http://s3tools.org/s3cmd&#34;&gt;http://s3tools.org/s3cmd&lt;/a&gt;)
for uploading files to S3.&lt;/p&gt;

&lt;p&gt;However I wanted something a little simpler, that generates the Helm index for me, without me having to do it by hand, and also
be able to host it myself in a Kubernetes cluster, so I wrote a  small &lt;code&gt;Go&lt;/code&gt; server called &lt;code&gt;Helmet&lt;/code&gt; to act as my helm
repository. It&amp;rsquo;s basically just a web server, to which you can upload chart files using something like &lt;code&gt;curl&lt;/code&gt; and it then
handles the repository indexing for you using helm in the backend.&lt;/p&gt;

&lt;p&gt;You can find out more about &lt;code&gt;Helmet&lt;/code&gt; at
&lt;a href=&#34;https://github.com/daemonza/helmet&#34;&gt;https://github.com/daemonza/helmet&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;using-a-helm-repository&#34;&gt;Using a Helm repository&lt;/h4&gt;

&lt;p&gt;Using &lt;code&gt;Helmet&lt;/code&gt; as our helm repository, let&amp;rsquo;s deploy it to our Kubernetes cluster and then add a chart.
And off course we can deploy &lt;code&gt;Helmet&lt;/code&gt; with Helm :D&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/daemonza/helmet.git; cd helmet/helmet-chart
helm package . --debug
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A helmet chart should be created, in my case it is &lt;code&gt;helmet-chart-0.0.1.tgz&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Deploy the same way we deployed the testapi.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm install helmet-chart-0.0.1.tgz --debug
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For this blog post, I deployed everything to Kubernetes Minikube(&lt;a href=&#34;https://github.com/kubernetes/minikube&#34;&gt;https://github.com/kubernetes/minikube&lt;/a&gt;)
So using minikube, let&amp;rsquo;s see how we can access &lt;code&gt;helmet&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HELMET=(`kubectl get services | awk &#39;/helmet/ {print $1}&#39;`)
minikube service $HELMET --url
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which gives me back &lt;code&gt;http://192.168.99.100:31162&lt;/code&gt;. Using this URL, let&amp;rsquo;s add it as a helm repository.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm repo add helmet http://192.168.99.100:31162/charts/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And confirm that our &lt;code&gt;helmet&lt;/code&gt; repo is there by running &lt;code&gt;helm repo list&lt;/code&gt;.
You should see the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME  	URL
stable	https://kubernetes-charts.storage.googleapis.com/
helmet	http://192.168.99.100:31162/charts/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now add our testapi chart to this &lt;code&gt;helmet&lt;/code&gt; repository with :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v -T testapi-chart-0.1.1.tgz -X PUT http://192.168.99.100:31162/upload/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can confirm that the chart is uploaded and the helm repo index got created by running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl http://192.168.99.100:31162/charts/index.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which will give us the following output :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
entries:
  testapi-chart:
  - apiVersion: v1
    created: 2017-03-01T12:11:09.746867088Z
    description: A Helm chart for Kubernetes
    digest: fe0c17d87b523c91cc59bd1e4d2f997defb2a215c4cc0fc02a1725922471e88a
    name: testapi-chart
    urls:
    - http://masked-macaw-helmet-char:1323/charts/testapi-chart-0.1.1.tgz
    version: 0.1.1
generated: 2017-03-01T12:11:09.746407601Z
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now search for the testapi, across all our repositories.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm search testapi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Expected output :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME                	VERSION	DESCRIPTION
helmet/testapi-chart	0.1.1  	A Helm chart for Kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s try searching for something else. Searching for Jenkins gives us this&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME          	VERSION	DESCRIPTION
stable/jenkins	0.1.14 	Open source continuous integration server. It s...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which we can see comes from the &lt;code&gt;stable&lt;/code&gt; repository. Very nice! Have I mentioned I love helm? :D
Now, to continue  let&amp;rsquo;s install the testapi from our &lt;code&gt;helmet&lt;/code&gt; repository :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm install helmet/testapi-chart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s how simple it is to use helm repositories. We can now use this &lt;code&gt;helmet&lt;/code&gt; helm repository from anywhere
upload charts to and deploy from.&lt;/p&gt;

&lt;h4 id=&#34;ci-cd&#34;&gt;CI / CD&lt;/h4&gt;

&lt;p&gt;Helm makes continuous integration and deployment a lot easier with Kubernetes. For example one strategy of doing it could
be, matching up your git branches to helm repositories.
Imagine you are busy building a application, and you have two branches (develop, master), master is your stable
branch and anything there could be deployed to a production kubernetes cluster.&lt;/p&gt;

&lt;p&gt;Example of your develop and stable helm repositories :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm develop add helmet http://repositoryone:31162/charts/
helm master add helmet http://repositorythree:31162/charts/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Write your code on a develop branch, and on a git push, Jenkins picks up that there was a change, build your code, run your
tests, and create a helm chart and uploads that chart to your &lt;code&gt;develop&lt;/code&gt; helm repository. From there you can then
easily deploy it into your development Kubernetes cluster (minikube for example). A push to master might trigger
Jenkins to deploy the code as a stable release to your master repository and then deploy it to your production kubernetes
cluster. Using a simple setup like this it becomes easy to deploy either develop level quality packages or stable
packages to kubernetes clusters.&lt;/p&gt;

&lt;p&gt;For Jenkins(&lt;a href=&#34;https://jenkins.io/&#34;&gt;https://jenkins.io/&lt;/a&gt;) you can use the this plugin(&lt;a href=&#34;https://github.com/jenkinsci/kubernetes-ci-plugin&#34;&gt;https://github.com/jenkinsci/kubernetes-ci-plugin&lt;/a&gt;) for
easy helm usage.&lt;/p&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;Helm makes doing reliable reproducible deployments ridiculously easy to Kubernetes, and I cannot recommend enough
to make it part of your standard way of working with Kubernetes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes nginx-ingress-controller</title>
      <link>https://daemonza.github.io/2017/02/13/kubernetes-nginx-ingress-controller/</link>
      <pubDate>Mon, 13 Feb 2017 09:42:32 +0100</pubDate>
      <author>werner.gillmer@gmail.com (Werner Gillmer)</author>
      <guid>https://daemonza.github.io/2017/02/13/kubernetes-nginx-ingress-controller/</guid>
      <description>

&lt;h4 id=&#34;introduction&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;In this post I will explain, how I expose applications running on Kubernetes clusters to the internet with the
help of Ingress controllers.&lt;/p&gt;

&lt;p&gt;But first a little bit about Kubernetes Ingresses and Services. On a very simplistic level a &lt;code&gt;Service&lt;/code&gt; is a
logical abstraction communication layer to pods. During normal operations pods get&amp;rsquo;s created, destroyed, scaled out, etc.&lt;/p&gt;

&lt;p&gt;A &lt;code&gt;Service&lt;/code&gt; make&amp;rsquo;s it easy to always connect to the pods by connecting to their &lt;code&gt;service&lt;/code&gt; which stays stable during the
pod life cycle. A important thing about &lt;code&gt;services&lt;/code&gt; are what their type is, it determines how the &lt;code&gt;service&lt;/code&gt; expose itself
to the cluster or the internet. Some of the &lt;code&gt;service&lt;/code&gt; types are :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ClusterIP
Your service is only expose internally to the cluster on the internal cluster IP. A example would be to
deploy Hasicorp&amp;rsquo;s vault and expose it only internally.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;NodePort
Expose the service on the EC2 Instance on the specified port. This will be exposed to the internet. Off course it
this all depends on your AWS Security group / VPN rules.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;LoadBalancer
Supported on Amazon and Google cloud, this creates the cloud providers your using load balancer. So on Amazon it creates
a ELB that points to your &lt;code&gt;service&lt;/code&gt; on your cluster.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ExternalName
Create a CNAME dns record to a external domain.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information about &lt;code&gt;Services&lt;/code&gt; look at &lt;a href=&#34;https://kubernetes.io/docs/user-guide/services/&#34;&gt;https://kubernetes.io/docs/user-guide/services/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A &lt;code&gt;Ingress&lt;/code&gt; is rules on how to access a &lt;code&gt;Service&lt;/code&gt; from the internet.&lt;/p&gt;

&lt;p&gt;For more information about &lt;code&gt;Ingresses&lt;/code&gt; look at
&lt;a href=&#34;https://kubernetes.io/docs/user-guide/ingress/&#34;&gt;https://kubernetes.io/docs/user-guide/ingress/&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;scenario&#34;&gt;Scenario&lt;/h4&gt;

&lt;p&gt;Imagine this scenario, you have a cluster running, on Amazon, you have multiple applications deployed to it, some are API&amp;rsquo;s some
are Java applications running inside something like Tomcat, and to add to the mix, you have a couple of static html pages sitting in a
Apache web server that serves as documentation for your api&amp;rsquo;s. All applications needs to have SSL, some of the api&amp;rsquo;s endpoints have changed,
but you still have to serve the old endpoint path, so you need to do some sort of path rewrite. How do you expose everything to the
internet? The obvious answer is create a  &lt;code&gt;type LoadBalancer&lt;/code&gt; service for each, but, then multiple ELB&amp;rsquo;s will be created, you have to
deal with SSL termination at each ELB, you have to CNAME your applications/api&amp;rsquo;s domain names to the right ELB&amp;rsquo;s, and in general just have
very little control over the ELB.&lt;/p&gt;

&lt;p&gt;Enter &lt;code&gt;Ingress Controllers&lt;/code&gt;. You deploy a &lt;code&gt;ingress controller&lt;/code&gt;, create a &lt;code&gt;type LoadBalancer&lt;/code&gt; service for it, and it sits and monitors
Kubernetes api server&amp;rsquo;s /ingresses endpoint and acts as a reverse proxy for the ingress rules it found there. You then deploy
your application and expose it&amp;rsquo;s service as a &lt;code&gt;type NodePort&lt;/code&gt;, and create ingress rules for it. The &lt;code&gt;ingress controller&lt;/code&gt; then
picks up the new deployed service and proxy traffic to it from outside.&lt;/p&gt;

&lt;p&gt;Following this setup, you only have one ELB then on Amazon, and a central place at the &lt;code&gt;ingress controller&lt;/code&gt; to manage the traffic coming
into your cluster to your applications.&lt;/p&gt;

&lt;h4 id=&#34;ingress-controllers&#34;&gt;Ingress controllers&lt;/h4&gt;

&lt;p&gt;Two of the more popular ones that I normally use, are Traefik and Nginx-ingress-controller. I like both, and for simple projects I prefer using Traefik
as it&amp;rsquo; easy to setup and use. However as the projects needs grow or for more complex projects, I reach for the nginx-ingress-controller.&lt;/p&gt;

&lt;p&gt;Couple of differences between the two, that normally helps me decide which one to pick for a project.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Traefik routes to the Kubernetes service, Nginx, by pass the service and routes to the pods. This allows nginx
to do sticky sessions if your application needs it.&lt;/li&gt;
&lt;li&gt;Traefik works with multiple backends (docker, swarm, marathon, etcd, consul, kubernetes, etc. etc.) See &lt;a href=&#34;https://docs.traefik.io/&#34;&gt;https://docs.traefik.io/&lt;/a&gt;
The nginx-ingress-controller is specfic for Kubernetes, you can off course build your own Nginx reverse proxy, perhaps with OpenResty
and get it to work with almost any backend, but that requires a significant time investment.&lt;/li&gt;
&lt;li&gt;The nginx-ingress-controller can handle websockets, Traefik does not.&lt;/li&gt;
&lt;li&gt;Traefik can do basic path manipulation by using &lt;code&gt;PathPrefixStrip&lt;/code&gt; but cannot do more complex path rewrites like Nginx.&lt;/li&gt;
&lt;li&gt;The nginx-ingress-controller is build on top of OpenResty (&lt;a href=&#34;https://openresty.org&#34;&gt;https://openresty.org&lt;/a&gt;). Using Lua you can easily extend Nginx capabilities
and mold it to do whatever you need it to do.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information look at :&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://traefik.io/&#34;&gt;https://traefik.io/&lt;/a&gt;
&lt;a href=&#34;https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx&#34;&gt;https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In general I prefer the nginx-ingress-controller, so for the rest of the article I will focus on it.&lt;/p&gt;

&lt;h4 id=&#34;setup&#34;&gt;Setup&lt;/h4&gt;

&lt;p&gt;Let&amp;rsquo;s setup a little demo &amp;lsquo;hello world&amp;rsquo; api on our  cluster and expose the api on the internet using
nginx-ingress-controller. Let&amp;rsquo;s deploy the &lt;code&gt;nginx-ingress-controller&lt;/code&gt; first.&lt;/p&gt;

&lt;p&gt;For the controller, the first thing we need to do is setup a default backend service for
nginx.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;default backend&lt;/code&gt; is the default service that nginx falls backs to if if cannot route a request
successfully. The &lt;code&gt;default backend&lt;/code&gt; needs to satisfy the following two requirements :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;serves a 404 page at /&lt;/li&gt;
&lt;li&gt;serves 200 on a /healthz&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See more here &lt;a href=&#34;https://github.com/kubernetes/contrib/tree/master/404-server&#34;&gt;https://github.com/kubernetes/contrib/tree/master/404-server&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s use the example &lt;code&gt;default backend&lt;/code&gt; from the nginx-ingress-controller github project&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f https://raw.githubusercontent.com/kubernetes/contrib/master/ingress/controllers/nginx/examples/default-backend.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we want to handle SSL requests (which we should always do) Nginx needs to have a default SSL certificate. This
get&amp;rsquo;s used for requests for there is not specified SSL certificate.&lt;/p&gt;

&lt;p&gt;Assuming you have SSL cert and key, create &lt;code&gt;secrets&lt;/code&gt; as follow, where tsl.key is the key name
and tsl.crt is your certificate and dhparam.pem is the pem file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create secret tls tls-certificate --key tls.key --cert tls.crt 
kubectl create secret generic tls-dhparam --from-file=dhparam.pem 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save the following to a file (e.g. nginx-ingress-controller.yml) :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
spec:
  type: LoadBalancer
  ports:
    - port: 80
      name: http
    - port: 443
      name: https
  selector:
    k8s-app: nginx-ingress-lb
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 2
  revisionHistoryLimit: 3
  template:
    metadata:
      labels:
        k8s-app: nginx-ingress-lb
    spec:
      terminationGracePeriodSeconds: 60
      containers:
        - name: nginx-ingress-controller
          image: gcr.io/google_containers/nginx-ingress-controller:0.8.3
          imagePullPolicy: Always
          readinessProbe:
            httpGet:
              path: /healthz
              port: 18080
              scheme: HTTP
          livenessProbe:
            httpGet:
              path: /healthz
              port: 18080
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 5
          args:
            - /nginx-ingress-controller
            - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
            - --default-ssl-certificate=$(POD_NAMESPACE)/tls-certificate
          # Use downward API
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - containerPort: 80
            - containerPort: 443
          volumeMounts:
            - name: tls-dhparam-vol
              mountPath: /etc/nginx-ssl/dhparam
            - name: nginx-template-volume
              mountPath: /etc/nginx/template
              readOnly: true
      volumes:
        - name: tls-dhparam-vol
          secret:
            secretName: tls-dhparam
        - name: nginx-template-volume
          configMap:
            name: nginx-template
            items:
            - key: nginx.tmpl
              path: nginx.tmpl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You might have noticed as well, we specify a &lt;code&gt;default-backend-service&lt;/code&gt;, this is used by nginx to route requests on
which it found not ingress rule match. We need to deploy this &lt;code&gt;default-backend-service&lt;/code&gt; before we deploy
nginx. We can use the example default backend yaml file from the Kubernetes ingress github repo. After the deployment
expose the &lt;code&gt;default-http-backend&lt;/code&gt; so that the nginx-ingress-controller can communicate with it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f https://raw.githubusercontent.com/kubernetes/contrib/master/ingress/controllers/nginx/examples/default-backend.yaml
kubectl expose rc default-http-backend --port=80 --target-port=8080 --name=default-http-backend
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now deploy &lt;code&gt;nginx-ingress-controller&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./nginx-ingress-controller.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What this command did was, it created a deployment with two replicas of the &lt;code&gt;nginx-ingress-controller&lt;/code&gt;
and a service for it of &lt;code&gt;type LoadBalancer&lt;/code&gt; which created a ELB for us on AWS.
Let&amp;rsquo;s confirm that. Get the service :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get services -o wide | grep nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should get something similar to the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nginx-ingress  100.62.254.11  aaba00ec3d35211r68caa0a32e7f202e-566d19265.eu-west-1.elb.amazonaws.com  80:32199/TCP,443:31772/TCP 25d  k8s-app=nginx-ingress-lb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This means the ELB on Amazon got created. Any domain&amp;rsquo;s who&amp;rsquo;s traffic we want to go to this
ingress controller should be CNAMED to this ELB hostname (&lt;code&gt;aaba00ec3d35211r68caa0a32e7f202e-566d19265.eu-west-1.elb.amazonaws.com&lt;/code&gt;)
For the rest of this post, I am going to assume api.daemonza.io is CNAMED to this ELB&lt;/p&gt;

&lt;p&gt;Make sure the deployment pods are up and running :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get pods | grep nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should get something similar to the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nginx-ingress-controller-1461575992-62j00   1/1       Running   0          30sec
nginx-ingress-controller-1461575992-eoqqb   1/1       Running   0          30sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using the ELB hostname that we got from the querying for the &lt;code&gt;nginx-ingress-controller&lt;/code&gt; service, make
sure that traffic is being routed.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -v aaba00ec3d35211r68caa0a32e7f202e-566d19265.eu-west-1.elb.amazonaws.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HTTP/1.1 404 Not Found
Server: nginx/1.11.3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the response should be&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;default backend - 404
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This means everything is working correctly and the ELB forwarded traffic to our
&lt;code&gt;nginx-ingress-controller&lt;/code&gt; and the &lt;code&gt;nginx-ingress-controller&lt;/code&gt; passed it along to the
&lt;code&gt;default-backend-service&lt;/code&gt; that we deployed.&lt;/p&gt;

&lt;h4 id=&#34;deploying-our-api-application&#34;&gt;Deploying our API application&lt;/h4&gt;

&lt;p&gt;I like Go, so I created a little API that we can deploy and test the ingress controller with.
This is the api we are going to deploy :
&lt;a href=&#34;https://hub.docker.com/r/daemonza/testapi/&#34;&gt;https://hub.docker.com/r/daemonza/testapi/&lt;/a&gt;
and the super simplistic source code is at &lt;a href=&#34;https://github.com/daemonza/testapi&#34;&gt;https://github.com/daemonza/testapi&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Create a file, let&amp;rsquo;s call it testapi.md with the following contents&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1 
kind: Deployment 
metadata:
  name: testapi 
  namespace: default 
  labels:
    application: testapi
spec:
  replicas: 3
  selector:
    matchLabels:
      application: testapi
  template:
    metadata:
      labels:
        application: testapi 
    spec:
      containers:
        - name: testapi
          image: daemonza/testapi:latest
          imagePullPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: testapi 
  namespace: default 
  labels:
    application: testapi 
spec:
  type: NodePort
  selector:
    application: testapi 
  ports:
  - port: 8080
    targetPort: 8080 
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: testapi
  namespace: default
  annotations:
    kubernetes.io/ingress.class: &amp;quot;nginx&amp;quot;
    ingress.kubernetes.io/ssl-redirect: &amp;quot;true&amp;quot;
spec:
  rules:
  - host: &amp;quot;api.daemonza.io&amp;quot;
    http:
      paths:
      - path: /testapi
        backend:
          serviceName: testapi
          servicePort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Quick run through of the above file. It consists of three parts, deployment, service, ingress.&lt;/p&gt;

&lt;p&gt;I normally split these into threes separate files, but for the sake of this blog post, let&amp;rsquo;s
keep it all in one file. For the nginx-ingress-controller we are mostly interested in
the &lt;code&gt;ingress&lt;/code&gt; part.&lt;/p&gt;

&lt;p&gt;In the form of &lt;code&gt;annotations&lt;/code&gt; it&amp;rsquo;s possible for us to pass some configuration through to the nginx-ingress-controller.
In the above example we say we want this to always be used with nginx-ingress-controller, in case we have multiple
controllers, and we always want to redirect HTTP to HTTPS.
Then we want to route all traffic where the host (virtual host) is api.daemonza.io but only on the &lt;code&gt;/testapi&lt;/code&gt; path, and
we want to route the traffic to the &lt;code&gt;testapi service&lt;/code&gt;, which in turn expose the &lt;code&gt;testapi deployment&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If we look at the equivalent simplified nginx configuration it would look similar to the following, where the &lt;code&gt;upstream&lt;/code&gt; is
&lt;code&gt;deployment&lt;/code&gt; with three replica&amp;rsquo;s.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    upstream default-testapi-8080 {
        server 100.96.3.148:8080 max_fails=0 fail_timeout=0;
        server 100.96.4.222:8080 max_fails=0 fail_timeout=0;
        server 100.96.4.223:8080 max_fails=0 fail_timeout=0;   
    }
    
    server {
            server_name api.daemonza.io;
            listen 80;

            location /testapi {
                   
                proxy_pass http://default-testapi-8080;
            }
            
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s test it.
Deploy with the following command&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./testapi.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see output similar to&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;deployment &amp;quot;testapi&amp;quot; created
service &amp;quot;testapi&amp;quot; created
ingress &amp;quot;testapi&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s check on the ingress rule&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get ingress testapi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see something similar to :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME      HOSTS             ADDRESS          PORTS     AGE
testapi   api.daemonza.io   51.232.218.155   80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s ask for a little more information with &lt;code&gt;describe&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl describe ingress testapi


Name:			testapi
Namespace:		default
Address:		52.214.207.195
Default backend:	default-http-backend:80 (&amp;lt;none&amp;gt;)
Rules:
  Host			Path	Backends
  ----			----	--------
  api.daemonza.io
    			/testapi 	testapi:8080 (&amp;lt;none&amp;gt;)
Annotations:
  ssl-redirect:	true
Events:
  FirstSeen	LastSeen	Count	From				SubObjectPath	Type		Reason	Message
  ---------	--------	-----	----				-------------	--------	------	-------
  2m		2m		1	{nginx-ingress-controller }			Normal		CREATE	default/testapi
  2m		2m		1	{nginx-ingress-controller }			Normal		CREATE	ip: 51.232.218.155
  2m		2m		1	{nginx-ingress-controller }			Normal		UPDATE	default/testapi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From this we can see that we asked the nginx-ingress-controller to route requests
for &lt;code&gt;api.daemonza.io&lt;/code&gt; on path &lt;code&gt;/testapi&lt;/code&gt; to our backend &lt;code&gt;testapi&lt;/code&gt;. Plain HTTP requests
will be redirected over to HTTPS and the testapi is scaled out to three replicas.&lt;/p&gt;

&lt;p&gt;Ok so with the testapi deployed, let&amp;rsquo;s see if everything works.
The test api expose the following endpoints :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PUT /put/:something
GET /get/:something
POST /post/:something
DELETE /get/:something
PATCH /patch/:something
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where :something is anything you want to put there.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s try a POST request&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -X POST http://api.daemonza.io/post/hello
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We should get http status code 301(moved permanently) back, this is because in our
testapi ingress specification, we added this annotation :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ingress.kubernetes.io/ssl-redirect: &amp;quot;true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which tells nginx to redirect http to https.
If we do the request again on https we should get status 200 back. Assuming that worked, let&amp;rsquo;s move on and add
some virtual hosts for our testapi.&lt;/p&gt;

&lt;h4 id=&#34;virtual-hosts&#34;&gt;Virtual hosts&lt;/h4&gt;

&lt;p&gt;Change the testapi.md ingress specification to look as follow, change the host domain name to a domain that you own
or setup them up in your local /etc/hosts file for testing.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: testapi
  namespace: default
  annotations:
    kubernetes.io/ingress.class: &amp;quot;nginx&amp;quot;
    ingress.kubernetes.io/ssl-redirect: &amp;quot;true&amp;quot;
spec:
  rules:
  - host: &amp;quot;blue.daemonza.io&amp;quot;
    http:
      paths:
      - path: /testapi
        backend:
          serviceName: testapi
          servicePort: 8080  
  - host: &amp;quot;green.daemonza.io&amp;quot;
    http:
      paths:
      - path: /testapi
        backend:
          serviceName: testapi
          servicePort: 8080  
  - host: &amp;quot;api.myfakedomain.io&amp;quot;
    http:
      paths:
      - path: /testapi
        backend:
          serviceName: testapi
          servicePort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also route multiple virtual hosts on different paths to the same backend service.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: testapi
  namespace: default
  annotations:
    kubernetes.io/ingress.class: &amp;quot;nginx&amp;quot;
    ingress.kubernetes.io/ssl-redirect: &amp;quot;true&amp;quot;
spec:
  rules:
  - host: &amp;quot;blue.daemonza.io&amp;quot;
    http:
      paths:
      - path: /testapi/get/blue
        backend:
          serviceName: testapi
          servicePort: 8080
  - host: &amp;quot;green.daemonza.io&amp;quot;
    http:
      paths:
      - path: /testapi/get/green
        backend:
          serviceName: testapi
          servicePort: 8080                 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the above the example we are routing two different virtual hosts, to the same API, but on different
api endpoints. We can then off course also do the opposite by having multiple little api &amp;ldquo;micro services&amp;rdquo;
and route traffic to the correct one by matching up the backend serviceName with the Path.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s test requests to our api on the api.myfakedomain.io virtual host we added.
Add api.myfakedomain.io to your /etc/hosts file in order to test with that domain.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl https://api.myfakedomain.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running that curl command, curls tells us &lt;code&gt;curl: (60) SSL certificate problem: Invalid certificate chain&lt;/code&gt;
which is correct as we don&amp;rsquo;t have the SSL certificate for that domain configured in nginx. Luckily through
ingress rules, we can specify a ssl certificate per virtual host as follow :&lt;/p&gt;

&lt;p&gt;First thing we need to do is create a Kubernetes secret containing our SSL certificate and key&lt;/p&gt;

&lt;p&gt;Add the following to a file, let&amp;rsquo;s call is apitest_ssl.md&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
kind: Secret
metadata:
  name: apitestsecret
  namespace: default
type: Opaque
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then deploy it to Kubernetes with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./apitest_ssl.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modify the apitest.md file ingress specification for api.myfakedomain.io to reference this secret as follow :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spec:
  tls:
  - hosts:
    - &amp;quot;api.myfakedomain.io&amp;quot;
    secretName: apitestsecret
  rules:
  - host: &amp;quot;api.myfakedomain.io&amp;quot;
    http:
      paths:
      - path: /testapi
        backend:
          serviceName: testapi
          servicePort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now if you ask for the api.myfakedomain.io ingress rules&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl describe ingress api.myfakedomain.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;you should get a similar reply to the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Name:			api.myfakedomain.io
Namespace:		default
Address:		52.223.113.173,52.224.52.153
Default backend:	default-http-backend:80 (&amp;lt;none&amp;gt;)
TLS:
  apitestsecret terminates api.myfakedomain.io
Rules:
  Host			Path	Backends
  ----			----	--------
  api.myfakedomain.io
    			/testapi 	apitest:8080 (&amp;lt;none&amp;gt;)
Annotations:
  ssl-redirect:	true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running the curl command now against &lt;a href=&#34;https://api.fakedomain.io&#34;&gt;https://api.fakedomain.io&lt;/a&gt; should work without any warnings about
insecure certificates.&lt;/p&gt;

&lt;h4 id=&#34;path-rewrites&#34;&gt;Path rewrites&lt;/h4&gt;

&lt;p&gt;Sometimes, there is a need to rewrite the path of a request to match up with the backend service. One such scenario might be,
a API got developed and deployed, got changed over time, but there is still a need to be backwards
compatibility on the API endpoints.&lt;/p&gt;

&lt;p&gt;So using out deployed apitest api, lets setup a path rewrite. We have a &lt;code&gt;/new/get&lt;/code&gt; api path, but the api does
not support it, it supports, &lt;code&gt;/get/new&lt;/code&gt; we now need to rewrite the path to that. Using a annotation with the ingress rules
we can rewrite the path as follow :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: apitest
  namespace: applications
  annotations:
    ingress.kubernetes.io/rewrite-target: /new/get
    kubernetes.io/ingress.class: &amp;quot;nginx&amp;quot;
spec:
  rules:
  - host: &amp;quot;api.daemonza.io&amp;quot;
    http:
      paths:
      - path: /get/new
        backend:
          serviceName: apitest
          servicePort: 8080
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;sticky-sessions&#34;&gt;Sticky sessions&lt;/h4&gt;

&lt;p&gt;Won&amp;rsquo;t it be nice if all applications got developed from the start with the 12-factor (&lt;a href=&#34;https://12factor.net/&#34;&gt;https://12factor.net/&lt;/a&gt;) methodology
in mind? Unfortunately that does not always happen, and there is times when you have to be able
to handle stateful application on your Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;Luckily for us the nginx-ingress-controller can handle sticky sessions as it bypass the service level and
route directly the pods.&lt;/p&gt;

&lt;p&gt;In order to get sticky sessions to work we need to create a configMap and enable it.&lt;/p&gt;

&lt;p&gt;Save the following in a file, for now call it nginx.conf.md&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
data:
  enable-sticky-sessions: &amp;quot;true&amp;quot;
kind: ConfigMap
metadata:
  name: nginx-ingress-controller-conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and load it with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f ./nginx.conf.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The controller will reload on configuration change.&lt;/p&gt;

&lt;p&gt;What this setting does it, instruct nginx to use the nginx-sticky-module-ng module
(&lt;a href=&#34;https://bitbucket.org/nginx-goodies/nginx-sticky-module-ng&#34;&gt;https://bitbucket.org/nginx-goodies/nginx-sticky-module-ng&lt;/a&gt;) that&amp;rsquo;s bundled with the controller
to handle all sticky sessions for us.&lt;/p&gt;

&lt;p&gt;This is how the generated nginx configuration looks after enabling sticky sessions.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;upstream default-testapi-8080 {
    sticky name=SERVERID httponly path=/;
    server 100.96.3.148:8080 max_fails=0 fail_timeout=0;
    server 100.96.4.222:8080 max_fails=0 fail_timeout=0;
    server 100.96.4.223:8080 max_fails=0 fail_timeout=0;   
}
    
server {
        server_name api.daemonza.io;
        listen 80;

        location /testapi {           
            proxy_pass http://default-testapi-8080;
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;proxy-protocol&#34;&gt;Proxy protocol&lt;/h4&gt;

&lt;p&gt;Lots of times you need to pass a user&amp;rsquo;s IP address / hostname through to your application. A example would
be, to have the hostname of the user in your application logs.&lt;/p&gt;

&lt;p&gt;By default if you deploy the nginx-ingress-controller on AWS behind a ELB, the ELB will not pass along the hostname
information, to solve this we need to enable &lt;code&gt;proxy protocol&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Add the following to your Nginx configMap.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;use-proxy-protocol: &amp;quot;true&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And in your nginx-ingress-controler service specification add the following annotation&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: &#39;*&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will make sure that the ELB that get&amp;rsquo;s created will have &lt;code&gt;proxy protocol&lt;/code&gt; enabled.
If you prefer not to change the ELB from a Kubernetes Service, you can configure it manually on the ELB
by following the documentation here : &lt;a href=&#34;http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html&#34;&gt;http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And excellent explanation of what proxy protocol is and how it works can be found at
&lt;a href=&#34;http://www.haproxy.org/download/1.8/doc/proxy-protocol.txt&#34;&gt;http://www.haproxy.org/download/1.8/doc/proxy-protocol.txt&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;custom-nginx-configuration&#34;&gt;Custom Nginx configuration&lt;/h4&gt;

&lt;p&gt;Sometimes there is a need to configure something in Nginx, that&amp;rsquo;s not possible through the nginx-ingress-controller
configMap, annotations or ingress rules. In such cases, it&amp;rsquo;s possible to edit &lt;code&gt;nginx.tmpl&lt;/code&gt; Go template.&lt;/p&gt;

&lt;p&gt;You can get the nginx.tmpl file from the nginx-ingress-controller Github repository or, copy it over from your
nginx-ingress-controller pod. It&amp;rsquo;s located at &lt;code&gt;/etc/nginx/template/nginx.tmpl&lt;/code&gt;. Make your changes to it
and then save it as a configMap as follow&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create configmap nginx-template --from-file=nginx.tmpl=./nginx.tmpl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then mount the nginx-template configMap as a volume in the your nginx-ingress-controller
specification.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;volumeMounts:
   - name: nginx-template-volume
     mountPath: /etc/nginx/template
      readOnly: true
volumes:
   - name: nginx-template-volume
     configMap:
       name: nginx-template
       items:
       - key: nginx.tmpl
         path: nginx.tmpl
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;The Kubernetes ingress specifications combined with the nginx-ingress-controller gives a incredible flexible and
powerful routing platform for your Kubernetes clusters. For more information about Kubernetes Ingress and the
Nginx-ingress-controller visit :&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/user-guide/ingress/&#34;&gt;https://kubernetes.io/docs/user-guide/ingress/&lt;/a&gt;
&lt;a href=&#34;https://github.com/nginxinc/kubernetes-ingress/tree/master/nginx-controller&#34;&gt;https://github.com/nginxinc/kubernetes-ingress/tree/master/nginx-controller&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes on AWS</title>
      <link>https://daemonza.github.io/2017/01/15/kubernetes-on-aws/</link>
      <pubDate>Sun, 15 Jan 2017 10:09:57 +0100</pubDate>
      <author>werner.gillmer@gmail.com (Werner Gillmer)</author>
      <guid>https://daemonza.github.io/2017/01/15/kubernetes-on-aws/</guid>
      <description>

&lt;p&gt;In this post I will be explaining how I setup Kubernetes clusters on Amazon.&lt;/p&gt;

&lt;p&gt;I use kops (&lt;a href=&#34;https://github.com/kubernetes/kops&#34;&gt;https://github.com/kubernetes/kops&lt;/a&gt;) to setup and manage my Kubernetes clusters. For my day job, I create
multiple Kubernetes stacks often on Amazon and nothing, so far, comes close to how well kops works for me. I briefly
flirted with kube-aws  (&lt;a href=&#34;https://github.com/coreos/kube-aws&#34;&gt;https://github.com/coreos/kube-aws&lt;/a&gt;) from CoreOS, and while it&amp;rsquo;s a good tool, Kops just works
better for me. Also there is something to be said for using a tool developed and used by the same guys that develop Kubernetes.&lt;/p&gt;

&lt;p&gt;There is also kubeadm (&lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/kubeadm/&#34;&gt;https://kubernetes.io/docs/getting-started-guides/kubeadm/&lt;/a&gt;), however it&amp;rsquo;s currently in alpha. Have not
used it myself before, but it&amp;rsquo;s definitely something I am keeping a eye on.&lt;/p&gt;

&lt;h4 id=&#34;installing-kops&#34;&gt;Installing kops&lt;/h4&gt;

&lt;p&gt;I use macOS so from here all instructions will be based on the assumption it&amp;rsquo;s being executed on macOS. It should
however work on Linux or at least be easily adapted to work on Linux.&lt;/p&gt;

&lt;p&gt;Install Go 1.7 and install kops from source. The pre-build releases of kops does not include a fix in the Go net lib
which seems to break things in kops every now and again, so we need to build it from source to get the fix.&lt;/p&gt;

&lt;p&gt;Install the following :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Go 1.7 or better from &lt;a href=&#34;https://golang.org/dl/&#34;&gt;https://golang.org/dl/&lt;/a&gt; or use brew&lt;/li&gt;
&lt;li&gt;kops - v1.4.4&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;go get -d k8s.io/kops
cd ${GOPATH}/src/k8s.io/kops/
git checkout release
make
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Test that kops works.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops version
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;setup&#34;&gt;Setup&lt;/h4&gt;

&lt;p&gt;It&amp;rsquo;s strictly not needed to install awscli, as you can do everything needed for setting up a kops created aws kubernetes
cluster through the Amazon web console. However, I find it useful and prefer it to the console.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;aws-cli/1.10.22 (&lt;a href=&#34;http://docs.aws.amazon.com/cli/latest/userguide/installing.html&#34;&gt;http://docs.aws.amazon.com/cli/latest/userguide/installing.html&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Configure your amazon credentials by running :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aws configure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure you don&amp;rsquo;t have &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt; and &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt; exported as environment variables in your
shell as it will take preference over the &lt;code&gt;aws configure&lt;/code&gt; credentials. You can off course skip the aws &lt;code&gt;aws configure&lt;/code&gt;
and just export your AWS credentials with the above mentioned environment variables.&lt;/p&gt;

&lt;p&gt;Create a S3 state store bucket for kops. This bucket get&amp;rsquo;s used by kops to store
information and configuration of all your Kubernetes clusters you create. Anyone with
Amazon credentials to the S3 bucket can create/modify and delete the your Kubernetes clusters, which
makes it very convenient to share this bucket with team members or if you move between computers, and not having to
move configuration, etc. with you.&lt;/p&gt;

&lt;p&gt;Creating a state store bucket in the eu-west-1 region of amazon.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aws s3api create-bucket --bucket my-state-store --region eu-west-1
export KOPS_STATE_STORE=s3://my-state-store
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Decide on a place to save your cluster kubeconfig and ssh keys locally. I like creating a directory structure
that mimics my my clusters. So for example if I am setting a development stack up for myself I would :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p ~/stacks/dev.daemonza.io
touch ~/stacks/dev.daemonza.io/kubeconfig 
EXPORT KUBECONFIG=&amp;quot;~/stacks/dev.daemonza.io/kubeconfig&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Change to this directory and create ssh keys. I normally put the keys in a separate &lt;code&gt;credentials&lt;/code&gt; directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir credentials
ssh-keygen -t rsa # save the keys in the credentials directory
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;create-cluster&#34;&gt;Create cluster&lt;/h4&gt;

&lt;p&gt;First thing is to decided on is the DNS setup for your cluster. It&amp;rsquo;s standard practice to use sub domains
to identify a cluster. E.g. dev.daemonza., qa.daemonza.io, prod.daemonza.io for the develop, QA and
production stacks. Following that convention let&amp;rsquo;s create our cluster under dev.daemonza.io.
Seeing as we are deploying to AWS, let&amp;rsquo;s use Amazon&amp;rsquo;s Route53 DNS service to handle the DNS side for us.&lt;/p&gt;

&lt;p&gt;If you have not already, point your root domain name, in this case daemonza.io to use Route53. Then
add a NS record to the daemonza.io for the sub domain where this cluster will be(dev.daemonza.io), with the
sub domains NS servers as values.&lt;/p&gt;

&lt;p&gt;First create the sub domain on route 53&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ID=$(uuidgen) &amp;amp;&amp;amp; aws route53 create-hosted-zone --name dev.daemonza.io --caller-reference $ID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Take note of the &amp;ldquo;NameServers&amp;rdquo; returned. It looks like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;NameServers&amp;quot;: [
    &amp;quot;ns-1225.awsdns-00.org&amp;quot;,
    &amp;quot;ns-1821.awsdns-44.co.uk&amp;quot;,
    &amp;quot;ns-820.awsdns-32.net&amp;quot;,
    &amp;quot;ns-143.awsdns-29.com&amp;quot;
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now using the above name servers, create a r53-ns-batch.json file. I save this file
in the locally created stack dir mentioned above.&lt;/p&gt;

&lt;p&gt;Example r53-ns-batch.json file using the above name servers&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    {
      &amp;quot;Comment&amp;quot;: &amp;quot;reference subdomain of dev.daemonza.io cluster&amp;quot;,
      &amp;quot;Changes&amp;quot;: [
        {
          &amp;quot;Action&amp;quot;: &amp;quot;CREATE&amp;quot;,
          &amp;quot;ResourceRecordSet&amp;quot;: {
            &amp;quot;Name&amp;quot;: &amp;quot;dev.daemonza.io&amp;quot;,
            &amp;quot;Type&amp;quot;: &amp;quot;NS&amp;quot;,
            &amp;quot;TTL&amp;quot;: 300,
            &amp;quot;ResourceRecords&amp;quot;: [
              {
                &amp;quot;Value&amp;quot;: &amp;quot;ns-1225.awsdns-00.org&amp;quot;
              },
              {
                &amp;quot;Value&amp;quot;: &amp;quot;ns-1821.awsdns-44.co.uk&amp;quot;
              },
              {
                &amp;quot;Value&amp;quot;: &amp;quot;ns-820.awsdns-32.net&amp;quot;
              },
              {
                &amp;quot;Value&amp;quot;: &amp;quot;ns-143.awsdns-29.com&amp;quot;
              }
            ]
          }
        }
      ]
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now using this r53-ns-batch.json file create a record in your parent domain (daemonza.io).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aws route53 change-resource-record-sets \
    --hosted-zone-id XXXXXXXXXXXX \
    --change-batch file://r53-ns-batch.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;XXXXXXXXXXXX being your your parent domain(daemonza.io) zone id. You can find your domain zone ID using&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aws route53 list-hosted-zones
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Use the following command to get the state of the DNS change&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;aws route53 get-change --id CHANGE_ID_FROM_PREVIOUS_COMMAND
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait here until status is INSYNC. It normally takes a couple of moments for me to get
the INSYNC.&lt;/p&gt;

&lt;p&gt;Now, we are ready to create the cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops create cluster --v=0 \
  --cloud=aws \
  --node-count 2 \
  --master-size=t2.medium \
  --master-zones=eu-west-1a \
  --zones eu-west-1a,eu-west-1b \
  --name=dev.daemonza.io \
  --node-size=m3.xlarge \
  --ssh-public-key=~/stacks/dev.daemonza.io/id_rsa.pub \
  --dns-zone dev.daemonza.io \
  2&amp;gt;&amp;amp;1 | tee /stacks/dev.daemonza.io/create_cluster.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A little explanation of the parameters passed to kops :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;--cloud&lt;/code&gt;
We want to install the cluster on Amazon&amp;rsquo;s AWS. At the moment only aws and google cloud is
supported.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--node-count&lt;/code&gt;
The number of worker nodes. This is the nodes running our applications.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--master-size&lt;/code&gt;
The EC2 instance type of the master node (manage Kubernetes)
Kubernetes recommends the following on master node sizing&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;For the master, for clusters of less than 5 nodes it will use an m3.medium, for 6-10 nodes it will use an m3.large;
for 11-100 nodes it will use an m3.xlarge.
For worker nodes, for clusters less than 50 nodes it will use a t2.micro, for clusters between 50 and 150 nodes it
will use a t2.small and for clusters with greater than 150 nodes it will use a t2.medium.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--master-zones&lt;/code&gt;
The AWS availability zone(&lt;a href=&#34;http://docs.aws.amazon.com/general/latest/gr/rande.html&#34;&gt;http://docs.aws.amazon.com/general/latest/gr/rande.html&lt;/a&gt;) where the master EC2
instance will be placed.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--zones&lt;/code&gt;
The AWS availability zones where the worker nodes EC2 instances will be places.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--name&lt;/code&gt;
The name you are giving to your stack. This is dev.daemonza.io for this example.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--node-size&lt;/code&gt;
The worker node EC2 instance type (&lt;a href=&#34;https://aws.amazon.com/ec2/instance-types/&#34;&gt;https://aws.amazon.com/ec2/instance-types/&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--ssh-public-key&lt;/code&gt;
This is the ssh key you generated in the previous command.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;--dns-zone&lt;/code&gt;
The fqdn of where your stack will be, I normally make this the same as the &lt;code&gt;name&lt;/code&gt; parameter, so
dev.daemonza.io in this case.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The output of the commmand I pipe to stdout, to monitor progress and activity while creating the stack and to a file
in case I missed something on stdout or want to reference something of the stack create in the future.&lt;/p&gt;

&lt;p&gt;This command generates your Kubernetes cluster configuration. Now run update (see below) to
create your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops update cluster dev.daemonza.io --yes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait a while for your cluster to come up. You can look in the AWS console at the EC2 instances
being created. It takes roughly around 10 mins or so for your cluster to become available.&lt;/p&gt;

&lt;p&gt;You can confirm the cluster is there by running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will take a while to work, and it will first start working on and off, as in you would get the nodes, or
some of the nodes back, or a error. Wait here until it returns all the nodes (in this case, two workers, and one master)
consistently. This step normally takes me around 15 to 20mins.&lt;/p&gt;

&lt;p&gt;When the cluster is up, I usually deploy the Kubernetes Dashboard (&lt;a href=&#34;https://github.com/kubernetes/dashboard&#34;&gt;https://github.com/kubernetes/dashboard&lt;/a&gt;). While I use
the kubectl command line tool probably 99% of the time, it&amp;rsquo;s nice to every now and again look at your cluster using the UI.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can now run kubectl proxy and access your cluster at
&lt;a href=&#34;http://127.0.0.1:8001/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard/#/deployment?namespace=_all&#34;&gt;http://127.0.0.1:8001/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard/#/deployment?namespace=_all&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;list-clusters&#34;&gt;List clusters&lt;/h4&gt;

&lt;p&gt;Kops makes it easy to manage multiple stacks. To get a list of all your current stacks
run the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops get clusters
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Example output :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;NAME			CLOUD	ZONES
dev.daemonza.io	aws	eu-west-1a,eu-west-1b
testing.daemonza.io	aws	eu-west-1a,eu-west-1b
production.daemonza.io	aws	eu-west-1a,eu-west-1b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see I have three stacks running. One being the stack (dev.daemonza.io) that we just created, and a testing
and production cluster.
From here we can get various information about a cluster. For example to get a description of the
EC2 instances that make up a cluster run :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops get instancegroups
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command will list the instances of the cluster to which your KUBECONFIG environment variable is
pointing. Earlier in this tutorial we created a kubeconfig file and exported KUECONFIG to that. The kops
create / update command filled out the kubeconfig file for us, which we are using now.&lt;/p&gt;

&lt;p&gt;Example output :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Using cluster from kubectl context: dev.daemonza.io

NAME			ROLE	MACHINETYPE	MIN	MAX	ZONES
master-eu-west-1a	Master	t2.medium	1	1	eu-west-1a
nodes			Node	m3.xlarge	2	2	eu-west-1a,eu-west-1b
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;modify-cluster&#34;&gt;Modify cluster&lt;/h4&gt;

&lt;p&gt;Kops makes it easy, and relative safe to modify a Kubernetes cluster with it&amp;rsquo;s rolling updates. Continuing from
our example above in the  cluster listing. Let&amp;rsquo;s modify our cluster to have three m3.xlarge worker nodes instead
of just two, and change the root volume size.&lt;/p&gt;

&lt;p&gt;Run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops edit instancegroups nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will open a configuration file in your default editor (set it with &lt;code&gt;export EDITOR=vim&lt;/code&gt; if you want vim as default)&lt;/p&gt;

&lt;p&gt;Example of how the configuration looks :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metadata:
  creationTimestamp: &amp;quot;2017-01-04T13:59:07Z&amp;quot;
  name: nodes
spec:
  associatePublicIp: true
  image: kope.io/k8s-1.4-debian-jessie-amd64-hvm-ebs-2016-10-21
  machineType: m3.xlarge
  maxSize: 2
  minSize: 2
  role: Node
  zones:
  - eu-west-1a
  - eu-west-1b                                                                                                                                                       13
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Change the maxSize value to 3 and keep minSize on 2, this will set the autoscale group on amazon
to scale to three worker nodes.
And for the root volume add the following for a 100gig volume of type gp2 (for more on EC2 volume types
see &lt;a href=&#34;http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html)&#34;&gt;http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html)&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rootVolumeSize: 100
rootVolumeType: gp2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With your end result looking like the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;metadata:
  creationTimestamp: &amp;quot;2017-01-04T13:59:07Z&amp;quot;
  name: nodes
spec:
  associatePublicIp: true
  image: kope.io/k8s-1.4-debian-jessie-amd64-hvm-ebs-2016-10-21
  machineType: m3.xlarge
  maxSize: 2
  minSize: 2
  role: Node
  rootVolumeSize: 100
  rootVolumeType: gp2
  zones:
  - eu-west-1a
  - eu-west-1b 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Confirm the changes is correct by running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops update cluster dev.daemonza.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If your happy with the changes run the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops update cluster dev.daemonza.io --yes
kops rolling-update cluster --yes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sit back and wait for your cluster changes to take affect.&lt;/p&gt;

&lt;h4 id=&#34;remove-cluster&#34;&gt;Remove cluster&lt;/h4&gt;

&lt;p&gt;Now to remove the cluster that you just created run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kops delete cluster dev.daemonza.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will print a list of amazon resources that will be removed.
Example :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TYPE		NAME				ID
dhcp-options	dev.daemonza.io	        	dopt-6627fc02
route-table 	dev.daemonza.io	    	    rtb-36fc1351
security-group	masters.dev.daemonza.io	    sg-e6e99a80
security-group	nodes.dev.daemonza.io	    sg-19e99a7f
subnet		    eu-west-1a.dev.daemonza.io	subnet-1cdbb344
subnet		    eu-west-1b.dev.daemonza.io	subnet-b17f51d5
vpc	        	dev.daemonza.io		        vpc-bbf7a5df

Must specify --yes to delete
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then if you are a 100% sure you want to remove the cluster, NOTE, there is no coming back
from this, add &amp;ndash;yes to the the command.&lt;/p&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;That&amp;rsquo;s it folks! Kops is a powerful tool to work with Kubernetes and we have barely scratch the
surface in this article. For more information, of what else you can do with Kops head over to
&lt;a href=&#34;https://github.com/kubernetes/kops&#34;&gt;https://github.com/kubernetes/kops&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>